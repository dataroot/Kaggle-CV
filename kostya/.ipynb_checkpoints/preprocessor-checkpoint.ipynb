{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataCreator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, pickle, json, re\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetCreator:\n",
    "    \"\"\"\n",
    "    Class that imports initial datasets and creates additional datasets for convenience\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path='../../data/', created=False):\n",
    "        # Add data_path to class properties\n",
    "        self.data_path = data_path\n",
    "        \n",
    "        # Import all initial datasets\n",
    "        self.emails = pd.read_csv(data_path + 'emails.csv')\n",
    "        self.questions = pd.read_csv(data_path + 'questions.csv')\n",
    "        self.professionals = pd.read_csv(data_path + 'professionals.csv')\n",
    "        self.comments = pd.read_csv(data_path + 'comments.csv')\n",
    "        self.tag_users = pd.read_csv(data_path + 'tag_users.csv')\n",
    "        self.group_memberships = pd.read_csv(data_path + 'group_memberships.csv')\n",
    "        self.tags = pd.read_csv(data_path + 'tags.csv')\n",
    "        self.students = pd.read_csv(data_path + 'students.csv')\n",
    "        self.groups = pd.read_csv(data_path + 'groups.csv')\n",
    "        self.tag_questions = pd.read_csv(data_path + 'tag_questions.csv')\n",
    "        self.matches = pd.read_csv(data_path + 'matches.csv')\n",
    "        self.answers = pd.read_csv(data_path + 'answers.csv')\n",
    "        self.school_memberships = pd.read_csv(data_path + 'school_memberships.csv')\n",
    "        \n",
    "        if created:\n",
    "            # Load additional datasets from disk\n",
    "            self.qa_data = pd.read_csv(self.data_path + 'qa_data.csv')\n",
    "            self.prof_data = pd.read_csv(self.data_path + 'prof_data.csv')\n",
    "            self.stud_data = pd.read_csv(self.data_path + 'stud_data.csv')\n",
    "        else:\n",
    "            # Create additional datasets and save them to disk\n",
    "            self.additional_datasets_creation()\n",
    "    \n",
    "    \n",
    "    def additional_datasets_creation(self):\n",
    "        \"\"\"\n",
    "        Creates additional datasets for futher processing and save them to disk.\n",
    "        \"\"\"\n",
    "        # Create temporary dataset for further processing\n",
    "        all_data = self.all_data_creation()\n",
    "        \n",
    "        # Create question-answer pairs dataset called qa_data\n",
    "        self.qa_data = self.qa_data_creation(all_data)\n",
    "        \n",
    "        # Create dataset called prof_data compirising data of professionals\n",
    "        # who answered at least one question\n",
    "        self.prof_data = self.prof_data_creation(all_data)\n",
    "        \n",
    "        # Create dataset called stud_data compirising data of students\n",
    "        # who asked at least one answered question\n",
    "        self.stud_data = self.stud_data_creation(all_data)\n",
    "        \n",
    "        # Save new datasets to disc\n",
    "        self.qa_data.to_csv(self.data_path + 'qa_data.csv', index=False)\n",
    "        self.prof_data.to_csv(self.data_path + 'prof_data.csv', index=False)\n",
    "        self.stud_data.to_csv(self.data_path + 'stud_data.csv', index=False)\n",
    "    \n",
    "    \n",
    "    def all_data_creation(self):\n",
    "        \"\"\"\n",
    "        Merges questions, answers, professionals and students datasets\n",
    "        to get temporary dataset for further processing\n",
    "        \"\"\"\n",
    "        # Merge questions with answers and delete not answered questions\n",
    "        all_data = self.questions.merge(self.answers, how='right', left_on='questions_id', right_on='answers_question_id')\n",
    "        \n",
    "        # Merge with professionals and students (students asked, professionals answered)\n",
    "        # Maybe change this in the future by taking care of professional who change status to students and vise versa\n",
    "        all_data = all_data.merge(self.professionals, how='inner', left_on='answers_author_id', right_on='professionals_id')\n",
    "        all_data = all_data.merge(self.students, how='inner', left_on='questions_author_id', right_on='students_id')\n",
    "        \n",
    "        # Transform dates from string representation to datetime object\n",
    "        all_data.answers_date_added = pd.to_datetime(all_data.answers_date_added)\n",
    "        all_data.questions_date_added = pd.to_datetime(all_data.questions_date_added)\n",
    "        \n",
    "        # Add questions_age feature, which represents amount of time\n",
    "        # from question emergence to a particular answer to that question\n",
    "        all_data['questions_age'] = all_data.answers_date_added - all_data.questions_date_added\n",
    "        \n",
    "        # Delete html tags and extra spaces from question and answer body\n",
    "        all_data.questions_body = (all_data.questions_body\n",
    "                                   .apply(lambda x: re.sub(r'(<[^>]*[/]?>|[\\r]?\\n)', ' ', str(x)))\n",
    "                                   .apply(lambda x: re.sub(r' +', ' ', x).strip()))\n",
    "        all_data.answers_body = (all_data.answers_body\n",
    "                                 .apply(lambda x: re.sub(r'(<[^>]*[/]?>|[\\r]?\\n)', ' ', str(x)))\n",
    "                                 .apply(lambda x: re.sub(r' +', ' ', x).strip()))\n",
    "        \n",
    "        # Count the number of words in question and answer body and add two new features\n",
    "        all_data['questions_body_length'] = all_data.questions_body.apply(lambda x: len(x.split(' ')))\n",
    "        all_data['answers_body_length'] = all_data.answers_body.apply(lambda x: len(x.split(' ')))\n",
    "        \n",
    "        return all_data\n",
    "    \n",
    "    \n",
    "    def qa_data_creation(self, all_data):\n",
    "        \"\"\"\n",
    "        Creates question-answer pairs dataset called qa_data_data\n",
    "        \"\"\"\n",
    "        # Temporary qa_data representation\n",
    "        qa_data = all_data\n",
    "        \n",
    "        # Select only unique professionals\n",
    "        temp = qa_data[['professionals_id', 'answers_date_added', 'answers_id']]\n",
    "        prof_unique = pd.DataFrame(temp.professionals_id.unique(), columns=['professionals_id'])\n",
    "        prof_unique = prof_unique.merge(self.professionals, how='left', on='professionals_id')\n",
    "        \n",
    "        # For every professional add a \"dummy\" question with answer date being professional's registration date\n",
    "        prof_unique['answers_id'] = list(None for _ in range(prof_unique.shape[0]))\n",
    "        prof_unique['answers_date_added'] = prof_unique['professionals_date_joined']\n",
    "        prof_unique = prof_unique[['professionals_id', 'answers_date_added', 'answers_id']]\n",
    "        \n",
    "        # Add \"dummy\" questions to all questions\n",
    "        temp = pd.concat([temp, prof_unique])\n",
    "        \n",
    "        # Sort by professionals and answer dates\n",
    "        temp = temp.sort_values(by=['professionals_id', 'answers_date_added']).reset_index(drop=True)\n",
    "        \n",
    "        # Get the sorted representation of the answers_date_added and shift the index down by one\n",
    "        # so that current question is aligned with previous question answer date\n",
    "        last_answer_date = pd.DataFrame({'professionals_last_answer_date': temp.answers_date_added})\n",
    "        last_answer_date.index += 1\n",
    "        \n",
    "        # Add the professionals_last_answer_date column to temp\n",
    "        temp = temp.merge(last_answer_date, left_index=True, right_index=True)\n",
    "        temp.dropna(subset=['answers_id'], inplace=True)\n",
    "        temp.drop(columns=['professionals_id', 'answers_date_added'], inplace=True)\n",
    "        \n",
    "        # Add professionals_last_answer_date column to qa_data \n",
    "        qa_data = qa_data.merge(temp, on='answers_id')\n",
    "        \n",
    "        # Transform dates from string representation to datetime object\n",
    "        qa_data.professionals_last_answer_date = pd.to_datetime(qa_data.professionals_last_answer_date)\n",
    "        \n",
    "        # Final qa_data representation\n",
    "        qa_data = qa_data[[\n",
    "            'students_id', 'questions_id', 'questions_title', 'questions_body',\n",
    "            'questions_body_length', 'questions_date_added', 'professionals_id',\n",
    "            'answers_id', 'answers_body', 'professionals_last_answer_date'\n",
    "        ]]\n",
    "        \n",
    "        return qa_data\n",
    "    \n",
    "    \n",
    "    def prof_data_creation(self, all_data):\n",
    "        \"\"\"\n",
    "        Creates dataset called prof_data compirising data of professionals who answered at least one question\n",
    "        \"\"\"\n",
    "        # Select only professionals who answered at least one question\n",
    "        active_professionals = pd.DataFrame({'professionals_id': all_data.professionals_id.unique()})\n",
    "        prof_data = self.professionals.merge(active_professionals, how='right', on='professionals_id')\n",
    "        \n",
    "        # Extract state or country from location\n",
    "        prof_data['professionals_state'] = prof_data['professionals_location'].apply(lambda loc: str(loc).split(', ')[-1])\n",
    "        \n",
    "        # Transform dates from string representation to datetime object\n",
    "        prof_data.professionals_date_joined = pd.to_datetime(prof_data.professionals_date_joined)\n",
    "        \n",
    "        # Count the number of answered questions by each professional\n",
    "        number_answered = all_data[['questions_id', 'professionals_id']].groupby('professionals_id').count()\n",
    "        number_answered = number_answered.rename({'questions_id': 'professionals_questions_answered'}, axis=1)\n",
    "        \n",
    "        # Add professionals_questions_answered feature to prof_data\n",
    "        prof_data = prof_data.merge(number_answered, left_on='professionals_id', right_index=True)\n",
    "        \n",
    "        # Get average question age for every professional among questions he answered\n",
    "        average_question_age = (\n",
    "            all_data.groupby('professionals_id')\n",
    "            .questions_age.mean(numeric_only=False)\n",
    "        )\n",
    "        average_question_age = pd.DataFrame({'professionals_average_question_age': average_question_age})\n",
    "        \n",
    "        # Add professionals_average_question_age feature to prof_data\n",
    "        prof_data = prof_data.merge(average_question_age, on='professionals_id')\n",
    "        \n",
    "        # Get all emails that every acting professional received\n",
    "        prof_emails_received = pd.merge(\n",
    "            prof_data[['professionals_id']], self.emails,\n",
    "            left_on='professionals_id', right_on='emails_recipient_id')\n",
    "        \n",
    "        # Get all questions every acting professional received in emails\n",
    "        prof_email_questions = prof_emails_received.merge(\n",
    "            self.matches, how='inner', left_on='emails_id', right_on='matches_email_id')\n",
    "        \n",
    "        # Get answered questions about which professionals were notified by email\n",
    "        questions_answered_from_emails = prof_email_questions.merge(\n",
    "            self.qa_data[['professionals_id', 'questions_id']],\n",
    "            left_on=['professionals_id', 'matches_question_id'],\n",
    "            right_on=['professionals_id', 'questions_id'])\n",
    "        \n",
    "        # Count the number of answered questions about which professionals were notified by email\n",
    "        email_activated = (questions_answered_from_emails\n",
    "                           .groupby('professionals_id')[['questions_id']].count()\n",
    "                           .rename(columns={'questions_id': 'professionals_email_activated'}))\n",
    "        \n",
    "        # Add professionals_email_activated feature to prof_data\n",
    "        # This feature is percent of answered questions about which professionals were notified by email\n",
    "        prof_data = prof_data.merge(email_activated, on='professionals_id', how='left')\n",
    "        prof_data.professionals_email_activated.fillna(0, inplace=True)\n",
    "        prof_data.professionals_email_activated /= prof_data.professionals_questions_answered\n",
    "        \n",
    "        return prof_data\n",
    "    \n",
    "    \n",
    "    def stud_data_creation(self, all_data):\n",
    "        \"\"\"\n",
    "        Creates dataset called stud_data compirising data of students who asked at least one answered question\n",
    "        \"\"\"\n",
    "        # Select only students who asked at least one answered question\n",
    "        active_students = pd.DataFrame({'students_id': all_data.students_id.unique()})\n",
    "        stud_data = self.students.merge(active_students, how='right', on='students_id')\n",
    "        \n",
    "        # Extract state or country from location\n",
    "        stud_data['students_state'] = stud_data['students_location'].apply(lambda loc: str(loc).split(', ')[-1])\n",
    "        \n",
    "        # Transform dates from string representation to datetime object\n",
    "        stud_data.students_date_joined = pd.to_datetime(stud_data.students_date_joined)\n",
    "        \n",
    "        # Count the number of asked questions by each student\n",
    "        number_asked = all_data[['questions_id', 'students_id']].groupby('students_id').count()\n",
    "        number_asked = number_asked.rename({'questions_id': 'students_questions_asked'}, axis=1)\n",
    "        \n",
    "        # Add students_questions_answered feature to stud_data\n",
    "        stud_data = stud_data.merge(number_asked, left_on='students_id', right_index=True)\n",
    "        \n",
    "        # Get average question age for every student among questions he asked that were answered\n",
    "        average_question_age = (\n",
    "            all_data.groupby('students_id')\n",
    "            .questions_age.mean(numeric_only=False)\n",
    "        )\n",
    "        average_question_age = pd.DataFrame({'students_average_question_age': average_question_age})\n",
    "        \n",
    "        # Add professionals_average_question_age feature to prof_data\n",
    "        stud_data = stud_data.merge(average_question_age, on='students_id')\n",
    "        \n",
    "        return stud_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator = DatasetCreator(created=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>students_id</th>\n",
       "      <th>questions_id</th>\n",
       "      <th>questions_title</th>\n",
       "      <th>questions_body</th>\n",
       "      <th>questions_body_length</th>\n",
       "      <th>questions_date_added</th>\n",
       "      <th>professionals_id</th>\n",
       "      <th>answers_id</th>\n",
       "      <th>answers_body</th>\n",
       "      <th>professionals_last_answer_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8f6f374ffd834d258ab69d376dd998f5</td>\n",
       "      <td>332a511f1569444485cf7a7a556a5e54</td>\n",
       "      <td>Teacher   career   question</td>\n",
       "      <td>What is a maths teacher? what is a maths teach...</td>\n",
       "      <td>14</td>\n",
       "      <td>2016-04-26 11:14:26</td>\n",
       "      <td>36ff3b3666df400f956f8335cf53e09e</td>\n",
       "      <td>4e5f01128cae4f6d8fd697cec5dca60c</td>\n",
       "      <td>Hi! You are asking a very interesting question...</td>\n",
       "      <td>2016-04-29 14:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8f6f374ffd834d258ab69d376dd998f5</td>\n",
       "      <td>7a0d4bc67b1c492fb06fe455b1c07faf</td>\n",
       "      <td>Teacher's Qualification</td>\n",
       "      <td>Hi I am doing my 10th Standard. What are the q...</td>\n",
       "      <td>27</td>\n",
       "      <td>2016-04-26 10:59:44</td>\n",
       "      <td>05ab77d4c6a141b999044ebbf5415b0d</td>\n",
       "      <td>334f6735d31e45589e43da5ae7056e50</td>\n",
       "      <td>It's helpful to take higher-level classes in S...</td>\n",
       "      <td>2018-03-08 18:23:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8f6f374ffd834d258ab69d376dd998f5</td>\n",
       "      <td>7a0d4bc67b1c492fb06fe455b1c07faf</td>\n",
       "      <td>Teacher's Qualification</td>\n",
       "      <td>Hi I am doing my 10th Standard. What are the q...</td>\n",
       "      <td>27</td>\n",
       "      <td>2016-04-26 10:59:44</td>\n",
       "      <td>05ab77d4c6a141b999044ebbf5415b0d</td>\n",
       "      <td>e5d66281cc314675b95ddbb799b75473</td>\n",
       "      <td>Essentially, treat them like human beings. You...</td>\n",
       "      <td>2018-03-08 18:23:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8f6f374ffd834d258ab69d376dd998f5</td>\n",
       "      <td>7a0d4bc67b1c492fb06fe455b1c07faf</td>\n",
       "      <td>Teacher's Qualification</td>\n",
       "      <td>Hi I am doing my 10th Standard. What are the q...</td>\n",
       "      <td>27</td>\n",
       "      <td>2016-04-26 10:59:44</td>\n",
       "      <td>58fa5e95fe9e480a9349bbb1d7faaddb</td>\n",
       "      <td>e5c0da2a29ff414fa76b9da6e86337fc</td>\n",
       "      <td>Check the link below. http://www.educationinfo...</td>\n",
       "      <td>2016-07-03 18:09:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>585ac233015447cc9e9a217044e515e1</td>\n",
       "      <td>0f1d6a4f276c4a05878dd48e03e52289</td>\n",
       "      <td>what kind of  college could i go  to for a soc...</td>\n",
       "      <td>I like soccer because i been playing sense i w...</td>\n",
       "      <td>27</td>\n",
       "      <td>2016-05-19 22:16:25</td>\n",
       "      <td>36ff3b3666df400f956f8335cf53e09e</td>\n",
       "      <td>f3519ab99a1a4a13a8a9ecb814287d2a</td>\n",
       "      <td>Hi Rodrigo! The important thing to remember if...</td>\n",
       "      <td>2016-07-31 15:10:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        students_id                      questions_id  \\\n",
       "0  8f6f374ffd834d258ab69d376dd998f5  332a511f1569444485cf7a7a556a5e54   \n",
       "1  8f6f374ffd834d258ab69d376dd998f5  7a0d4bc67b1c492fb06fe455b1c07faf   \n",
       "2  8f6f374ffd834d258ab69d376dd998f5  7a0d4bc67b1c492fb06fe455b1c07faf   \n",
       "3  8f6f374ffd834d258ab69d376dd998f5  7a0d4bc67b1c492fb06fe455b1c07faf   \n",
       "4  585ac233015447cc9e9a217044e515e1  0f1d6a4f276c4a05878dd48e03e52289   \n",
       "\n",
       "                                     questions_title  \\\n",
       "0                        Teacher   career   question   \n",
       "1                            Teacher's Qualification   \n",
       "2                            Teacher's Qualification   \n",
       "3                            Teacher's Qualification   \n",
       "4  what kind of  college could i go  to for a soc...   \n",
       "\n",
       "                                      questions_body  questions_body_length  \\\n",
       "0  What is a maths teacher? what is a maths teach...                     14   \n",
       "1  Hi I am doing my 10th Standard. What are the q...                     27   \n",
       "2  Hi I am doing my 10th Standard. What are the q...                     27   \n",
       "3  Hi I am doing my 10th Standard. What are the q...                     27   \n",
       "4  I like soccer because i been playing sense i w...                     27   \n",
       "\n",
       "  questions_date_added                  professionals_id  \\\n",
       "0  2016-04-26 11:14:26  36ff3b3666df400f956f8335cf53e09e   \n",
       "1  2016-04-26 10:59:44  05ab77d4c6a141b999044ebbf5415b0d   \n",
       "2  2016-04-26 10:59:44  05ab77d4c6a141b999044ebbf5415b0d   \n",
       "3  2016-04-26 10:59:44  58fa5e95fe9e480a9349bbb1d7faaddb   \n",
       "4  2016-05-19 22:16:25  36ff3b3666df400f956f8335cf53e09e   \n",
       "\n",
       "                         answers_id  \\\n",
       "0  4e5f01128cae4f6d8fd697cec5dca60c   \n",
       "1  334f6735d31e45589e43da5ae7056e50   \n",
       "2  e5d66281cc314675b95ddbb799b75473   \n",
       "3  e5c0da2a29ff414fa76b9da6e86337fc   \n",
       "4  f3519ab99a1a4a13a8a9ecb814287d2a   \n",
       "\n",
       "                                        answers_body  \\\n",
       "0  Hi! You are asking a very interesting question...   \n",
       "1  It's helpful to take higher-level classes in S...   \n",
       "2  Essentially, treat them like human beings. You...   \n",
       "3  Check the link below. http://www.educationinfo...   \n",
       "4  Hi Rodrigo! The important thing to remember if...   \n",
       "\n",
       "  professionals_last_answer_date  \n",
       "0            2016-04-29 14:15:00  \n",
       "1            2018-03-08 18:23:01  \n",
       "2            2018-03-08 18:23:36  \n",
       "3            2016-07-03 18:09:58  \n",
       "4            2016-07-31 15:10:27  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creator.qa_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(DatasetCreator):\n",
    "    \"\"\"\n",
    "    Class for qa_data, prof_data and stud_data feature preprocessing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, created=False):\n",
    "        \"\"\"\n",
    "        Initializes DatasetCreator class and loads existing\n",
    "        preprocessors that were already fit to data\n",
    "        \"\"\"\n",
    "        # Initialize DatasetCreator\n",
    "        super().__init__(created=created)\n",
    "        \n",
    "        # Load existing preprocessors that were already fit to data\n",
    "        if os.path.isfile('preprocessors.pickle'):\n",
    "            with open('preprocessors.pickle', 'rb') as file:\n",
    "                self.pp = pickle.load(file)\n",
    "        else:\n",
    "            self.pp = {}\n",
    "        \n",
    "        # Load file that contains number of categories for categorical features\n",
    "        with open('cat_features.json') as f:\n",
    "            self.cat_features = json.load(f)\n",
    "        \n",
    "        # Carry out preprocessing of all datasets\n",
    "        self.qa_data_preprocessing()\n",
    "        self.prof_data_preprocessing()\n",
    "        self.stud_data_preprocessing()\n",
    "    \n",
    "    \n",
    "    def qa_data_preprocessing(self):\n",
    "        \"\"\"\n",
    "        Preprocesses qa_data dataset\n",
    "        \"\"\"\n",
    "        # Preprocess datetime and timedelta features\n",
    "        Preprocessor.datetime(self.qa_data, 'questions_date_added', hour=True)\n",
    "        Preprocessor.datetime(self.qa_data, 'professionals_last_answer_date', hour=True)\n",
    "        \n",
    "        # Preprocess numerical features\n",
    "        for feature in [\n",
    "            'questions_date_added_time', 'questions_date_added_doy_sin',\n",
    "            'professionals_last_answer_date_time', 'professionals_last_answer_date_dow',\n",
    "            'questions_body_length'\n",
    "        ]:\n",
    "            Preprocessor.numerical(self.qa_data, feature, self.pp)\n",
    "    \n",
    "    \n",
    "    def prof_data_preprocessing(self):\n",
    "        \"\"\"\n",
    "        Preprocesses prof_data dataset\n",
    "        \"\"\"\n",
    "        # Preprocess datetime and timedelta features\n",
    "        Preprocessor.datetime(self.prof_data, 'professionals_date_joined')\n",
    "        Preprocessor.timedelta(self.prof_data, 'professionals_average_question_age')\n",
    "        \n",
    "        # Preprocess numerical features\n",
    "        for feature in [\n",
    "            'professionals_questions_answered', 'professionals_date_joined_time',\n",
    "            'professionals_date_joined_dow', 'professionals_average_question_age'\n",
    "        ]:\n",
    "            Preprocessor.numerical(self.prof_data, feature, self.pp)\n",
    "        \n",
    "        # Will need textual representation of industry in BatchGenerator\n",
    "        self.prof_data['professionals_industry_textual'] = self.prof_data['professionals_industry']\n",
    "        \n",
    "        # Preprocess categorical features\n",
    "        Preprocessor.categorical(\n",
    "            self.prof_data, 'professionals_industry',\n",
    "            self.cat_features['n_cats']['prof']['professionals_industry'],\n",
    "            self.pp, oblige_fit=True\n",
    "        )\n",
    "        Preprocessor.categorical(\n",
    "            self.prof_data, 'professionals_location',\n",
    "            self.cat_features['n_cats']['prof']['professionals_location'],\n",
    "            self.pp, oblige_fit=True\n",
    "        )\n",
    "        Preprocessor.categorical(\n",
    "            self.prof_data, 'professionals_state',\n",
    "            self.cat_features['n_cats']['prof']['professionals_state'],\n",
    "            self.pp, oblige_fit=True\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def stud_data_preprocessing(self):\n",
    "        \"\"\"\n",
    "        Preprocesses stud_data dataset\n",
    "        \"\"\"\n",
    "        # Preprocess datetime and timedelta features\n",
    "        Preprocessor.datetime(self.stud_data, 'students_date_joined')\n",
    "        Preprocessor.timedelta(self.stud_data, 'students_average_question_age')\n",
    "        \n",
    "        # Preprocess numerical features\n",
    "        for feature in [\n",
    "            'students_questions_asked', 'students_date_joined_time',\n",
    "            'students_date_joined_dow', 'students_average_question_age'\n",
    "        ]:\n",
    "            Preprocessor.numerical(self.stud_data, feature, self.pp)\n",
    "        \n",
    "        # Preprocess categorical features\n",
    "        Preprocessor.categorical(\n",
    "            self.stud_data, 'students_location',\n",
    "            self.cat_features['n_cats']['ques']['students_location'],\n",
    "            self.pp, oblige_fit=True\n",
    "        )\n",
    "        Preprocessor.categorical(\n",
    "            self.stud_data, 'students_state',\n",
    "            self.cat_features['n_cats']['ques']['students_state'],\n",
    "            self.pp, oblige_fit=True\n",
    "        )\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def datetime(df: pd.DataFrame, feature: str, hour: bool = False):\n",
    "        \"\"\"\n",
    "        Generates a bunch of new datetime features and drops the original feature inplace\n",
    "\n",
    "        :param df: Data to work with.\n",
    "        :param feature: Name of a column in df that contains date.\n",
    "        :param hour: Whether feature contains time.\n",
    "        \"\"\"\n",
    "        df[feature] = pd.to_datetime(df[feature])\n",
    "\n",
    "        df[feature + '_time'] = df[feature].apply(lambda d: d.year + d.dayofyear / 365)\n",
    "        df[feature + '_doy_sin'] = df[feature].apply(lambda d: np.sin(2 * np.pi * d.dayofyear / 365))\n",
    "        df[feature + '_doy_cos'] = df[feature].apply(lambda d: np.cos(2 * np.pi * d.dayofyear / 365))\n",
    "        df[feature + '_dow'] = df[feature].apply(lambda d: d.weekday())\n",
    "\n",
    "        if hour:\n",
    "            df[feature + '_hour_sin'] = df[feature].apply(lambda d: np.sin(2 * np.pi * (d.hour + d.minute / 60) / 24))\n",
    "            df[feature + '_hour_cos'] = df[feature].apply(lambda d: np.cos(2 * np.pi * (d.hour + d.minute / 60) / 24))\n",
    "\n",
    "        df.drop(columns=feature, inplace=True)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def timedelta(df: pd.DataFrame, feature: str):\n",
    "        \"\"\"\n",
    "        Generates the new timedelta feature\n",
    "\n",
    "        :param df: Data to work with.\n",
    "        :param feature: Name of a column in df that contains timedelta.\n",
    "        \"\"\"\n",
    "        df[feature] = pd.to_timedelta(df[feature])\n",
    "\n",
    "        df[feature] = df[feature] / pd.Timedelta(\"1 day\")\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_preprocessor(fit_data: np.array, feature: str, base, pp: dict, oblige_fit: bool):\n",
    "        \"\"\"\n",
    "        Creates new preprocessor having class base or uses existing one in preprocessors.pickle\n",
    "        Returns this preprocessor\n",
    "\n",
    "        :param fit_data: NumPy array of data to fit new preprocessor.\n",
    "        :param feature: Feature name to search for in preprocessors.pickle.\n",
    "        :param base: Preprocessor's class.\n",
    "        :param pp: Object with preprocessors.\n",
    "        :param oblige_fit: Whether to fit new preprocessor on feature even if there already exists one.\n",
    "        :returns: Preprocessor object.\n",
    "        \"\"\"    \n",
    "        if feature in pp and not oblige_fit:\n",
    "            preproc = pp[feature]\n",
    "        else:\n",
    "            preproc = base()\n",
    "            preproc.fit(fit_data)\n",
    "            pp[feature] = preproc\n",
    "            with open('preprocessors.pickle', 'wb') as file:\n",
    "                pickle.dump(pp, file)\n",
    "        return preproc\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def numerical(df: pd.DataFrame, feature: str, pp: dict, oblige_fit: bool = False):\n",
    "        \"\"\"\n",
    "        Transforms via StandardScaler\n",
    "\n",
    "        :param df: Data to work with.\n",
    "        :param feature: Name of a column in df that contains numerical data.\n",
    "        :param pp: Object with preprocessors.\n",
    "        :param oblige_fit: Whether to fit new StandardScaler on feature even if there already exists one.\n",
    "        \"\"\"\n",
    "        fit_data = df[feature].values.reshape(-1, 1).astype('float64')\n",
    "        sc = Preprocessor._get_preprocessor(fit_data, feature, StandardScaler, pp, oblige_fit)\n",
    "        df[feature] = sc.transform(fit_data)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def categorical(df: pd.DataFrame, feature: str, n: int, pp: dict, oblige_fit: bool = False):\n",
    "        \"\"\"\n",
    "        Encodes top n most popular values with different labels from 0 to n-1,\n",
    "        remaining values with n and NaNs with n+1\n",
    "\n",
    "        :param df: Data to work with.\n",
    "        :param feature: Name of a column in df that contains categorical data.\n",
    "        :param n: Number of top by popularity values to move in separate categories.\n",
    "                  0 to encode everything with different labels.\n",
    "        :param pp: Object with preprocessors.\n",
    "        :param oblige_fit: Whether to fit new LabelEncoder on feature even if there already exists one.\n",
    "        \"\"\"\n",
    "        vc = df[feature].value_counts()\n",
    "        n = len(vc) if n == 0 else n\n",
    "\n",
    "        top = set(vc[:n].index)\n",
    "        isin_top = df[feature].isin(top)\n",
    "\n",
    "        fit_data = df.loc[isin_top, feature]\n",
    "        le = Preprocessor._get_preprocessor(fit_data, feature, LabelEncoder, pp, oblige_fit)\n",
    "\n",
    "        isin_le = df[feature].isin(set(le.classes_))\n",
    "        df.loc[isin_le, feature] = le.transform(df.loc[isin_le, feature])\n",
    "\n",
    "        bottom = set(vc.index) - set(le.classes_)\n",
    "        isin_bottom = df[feature].isin(bottom)\n",
    "        df.loc[isin_bottom, feature] = n\n",
    "        df[feature].fillna(n + 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pp = Preprocessor(created=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>students_id</th>\n",
       "      <th>questions_id</th>\n",
       "      <th>questions_title</th>\n",
       "      <th>questions_body</th>\n",
       "      <th>questions_body_length</th>\n",
       "      <th>professionals_id</th>\n",
       "      <th>answers_id</th>\n",
       "      <th>answers_body</th>\n",
       "      <th>questions_date_added_time</th>\n",
       "      <th>questions_date_added_doy_sin</th>\n",
       "      <th>questions_date_added_doy_cos</th>\n",
       "      <th>questions_date_added_dow</th>\n",
       "      <th>questions_date_added_hour_sin</th>\n",
       "      <th>questions_date_added_hour_cos</th>\n",
       "      <th>professionals_last_answer_date_time</th>\n",
       "      <th>professionals_last_answer_date_doy_sin</th>\n",
       "      <th>professionals_last_answer_date_doy_cos</th>\n",
       "      <th>professionals_last_answer_date_dow</th>\n",
       "      <th>professionals_last_answer_date_hour_sin</th>\n",
       "      <th>professionals_last_answer_date_hour_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8f6f374ffd834d258ab69d376dd998f5</td>\n",
       "      <td>332a511f1569444485cf7a7a556a5e54</td>\n",
       "      <td>Teacher   career   question</td>\n",
       "      <td>What is a maths teacher? what is a maths teach...</td>\n",
       "      <td>-0.683257</td>\n",
       "      <td>36ff3b3666df400f956f8335cf53e09e</td>\n",
       "      <td>4e5f01128cae4f6d8fd697cec5dca60c</td>\n",
       "      <td>Hi! You are asking a very interesting question...</td>\n",
       "      <td>-0.401803</td>\n",
       "      <td>1.017302</td>\n",
       "      <td>-0.428892</td>\n",
       "      <td>1</td>\n",
       "      <td>0.199368</td>\n",
       "      <td>-0.979925</td>\n",
       "      <td>-0.646438</td>\n",
       "      <td>0.880012</td>\n",
       "      <td>-0.474951</td>\n",
       "      <td>0.771827</td>\n",
       "      <td>-0.55557</td>\n",
       "      <td>-0.83147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        students_id                      questions_id  \\\n",
       "0  8f6f374ffd834d258ab69d376dd998f5  332a511f1569444485cf7a7a556a5e54   \n",
       "\n",
       "               questions_title  \\\n",
       "0  Teacher   career   question   \n",
       "\n",
       "                                      questions_body  questions_body_length  \\\n",
       "0  What is a maths teacher? what is a maths teach...              -0.683257   \n",
       "\n",
       "                   professionals_id                        answers_id  \\\n",
       "0  36ff3b3666df400f956f8335cf53e09e  4e5f01128cae4f6d8fd697cec5dca60c   \n",
       "\n",
       "                                        answers_body  \\\n",
       "0  Hi! You are asking a very interesting question...   \n",
       "\n",
       "   questions_date_added_time  questions_date_added_doy_sin  \\\n",
       "0                  -0.401803                      1.017302   \n",
       "\n",
       "   questions_date_added_doy_cos  questions_date_added_dow  \\\n",
       "0                     -0.428892                         1   \n",
       "\n",
       "   questions_date_added_hour_sin  questions_date_added_hour_cos  \\\n",
       "0                       0.199368                      -0.979925   \n",
       "\n",
       "   professionals_last_answer_date_time  \\\n",
       "0                            -0.646438   \n",
       "\n",
       "   professionals_last_answer_date_doy_sin  \\\n",
       "0                                0.880012   \n",
       "\n",
       "   professionals_last_answer_date_doy_cos  professionals_last_answer_date_dow  \\\n",
       "0                               -0.474951                            0.771827   \n",
       "\n",
       "   professionals_last_answer_date_hour_sin  \\\n",
       "0                                 -0.55557   \n",
       "\n",
       "   professionals_last_answer_date_hour_cos  \n",
       "0                                 -0.83147  "
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.qa_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BatchGenerator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.utils import Sequence\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from utils import TextProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator(Sequence):\n",
    "    \"\"\"\n",
    "    Generates batches of data to feed into the model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pp: Preprocessor, batch_size: int = 50, shuffle: bool = True):\n",
    "        \"\"\"\n",
    "        Loads required datasets from pp, batch_size and shuffle parameters\n",
    "        \"\"\"\n",
    "        self.qa_data = pp.qa_data.merge(pp.stud_data, on='students_id')\n",
    "        self.prof_data = pp.prof_data\n",
    "        \n",
    "        # Select unique professionals from the ones that answered at least one question\n",
    "        self.unique_profs = pp.prof_data.professionals_id.unique()\n",
    "        \n",
    "        #----------------------------------------------------------------------------\n",
    "        #               INTEGRATION WITH NIKITA'S BATCH GENERATOR\n",
    "        #----------------------------------------------------------------------------\n",
    "        \n",
    "        # Load required datasets (their names are left as they were in Nikita's batch generator)\n",
    "        tag_que = pp.tag_questions\n",
    "        tags = pp.tags\n",
    "        pro = pp.prof_data\n",
    "        que = pp.qa_data\n",
    "        \n",
    "        # Import precomputed embeddings\n",
    "        with open('tags_embs.pickle', 'rb') as file:\n",
    "            self.tag_emb = pickle.load(file)\n",
    "        with open('industries_embs.pickle', 'rb') as file:\n",
    "            self.ind_emb = pickle.load(file)\n",
    "        \n",
    "        # Preprocess professionals industries\n",
    "        self.tp = TextProcessor()\n",
    "        pro['professionals_industry_textual'] = (pro['professionals_industry_textual']\n",
    "                                                 .apply(self.tp.process)\n",
    "                                                 .apply(lambda x: ' '.join(x)))\n",
    "        \n",
    "        # Map professionals_id to professionals_industry_textual\n",
    "        self.pro_ind = {row['professionals_id']: row['professionals_industry_textual'] for i, row in pro.iterrows()}\n",
    "        \n",
    "        # Create string of tags for every question\n",
    "        que_tags = (que.merge(tag_que, left_on='questions_id', right_on='tag_questions_question_id')\n",
    "                       .merge(tags, left_on='tag_questions_tag_id', right_on='tags_tag_id'))\n",
    "        que_tags = (que_tags[['questions_id', 'tags_tag_name']]\n",
    "                    .groupby('questions_id', as_index=False)\n",
    "                    .aggregate(lambda x: ' '.join(x)))\n",
    "        \n",
    "        # Map questions_id to string of tags\n",
    "        self.que_tag = {row['questions_id']: row['tags_tag_name'].split() for i, row in que_tags.iterrows()}\n",
    "        \n",
    "        #----------------------------------------------------------------------------\n",
    "        \n",
    "        # Set batch_size and shuffle parameters\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # Initial shuffle \n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Denotes the number of batches per epoch\n",
    "        \"\"\"\n",
    "        return self.qa_data.shape[0] // (self.batch_size)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generates one batch of data\n",
    "        \"\"\"\n",
    "        # Positive batch is selected by index\n",
    "        positive_batch = self.qa_data.iloc[index * self.batch_size : (index + 1) * self.batch_size, :]\n",
    "        negative_batch = positive_batch\n",
    "        \n",
    "        # Choose random professionals for negative batch\n",
    "        cur_profs = negative_batch.professionals_id\n",
    "        new_profs = np.random.choice(self.unique_profs, self.batch_size)\n",
    "        \n",
    "        # Check if all professionals from negative batch are different from true professionals\n",
    "        while np.sum(cur_profs == new_profs) > 0:\n",
    "            # If not (tiny probability), resample random professionals\n",
    "            new_profs = np.random.choice(self.unique_profs, self.batch_size)\n",
    "        \n",
    "        # Assign random professionals to negative batch\n",
    "        negative_batch.assign(professionals_id=new_profs)\n",
    "        \n",
    "        # Concatenate positive and negative batches into a single batch\n",
    "        single_batch = pd.concat([positive_batch, negative_batch])\n",
    "        \n",
    "        # Add professionals data to single_batch\n",
    "        single_batch = single_batch.merge(self.prof_data, on='professionals_id')\n",
    "        \n",
    "        # Select statistical question features\n",
    "        x_que_features = single_batch[[\n",
    "            'students_location', 'students_state', 'students_questions_asked',\n",
    "            'students_average_question_age', 'students_date_joined_time',\n",
    "            'students_date_joined_doy_sin', 'students_date_joined_doy_cos',\n",
    "            'students_date_joined_dow', 'questions_date_added_time',\n",
    "            'questions_date_added_doy_sin', 'questions_date_added_doy_cos',\n",
    "            'questions_date_added_dow', 'questions_date_added_hour_sin',\n",
    "            'questions_date_added_hour_cos', 'questions_body_length',\n",
    "        ]].values\n",
    "        \n",
    "        # Select statistical professional features\n",
    "        x_pro_features = single_batch[[\n",
    "            'professionals_industry', 'professionals_location', 'professionals_state',\n",
    "            'professionals_questions_answered', 'professionals_average_question_age',\n",
    "            'professionals_date_joined_time', 'professionals_date_joined_doy_sin',\n",
    "            'professionals_date_joined_doy_cos', 'professionals_date_joined_dow',\n",
    "            'professionals_last_answer_date_time', 'professionals_last_answer_date_doy_sin',\n",
    "            'professionals_last_answer_date_doy_cos', 'professionals_last_answer_date_dow',\n",
    "            'professionals_last_answer_date_hour_sin', 'professionals_last_answer_date_hour_cos',\n",
    "            'professionals_email_activated',\n",
    "        ]].values\n",
    "        \n",
    "        #----------------------------------------------------------------------------\n",
    "        #               INTEGRATION WITH NIKITA'S BATCH GENERATOR\n",
    "        #----------------------------------------------------------------------------\n",
    "        \n",
    "        # Extract embeddings from batch questions and professionals\n",
    "        x_que_embeddings, x_pro_embeddings = self.__convert(\n",
    "            single_batch[['questions_id', 'professionals_id']].values)\n",
    "        \n",
    "        # Stack statistical features and embeddings\n",
    "        x_que = np.hstack((x_que_features, x_que_embeddings))\n",
    "        x_pro = np.hstack((x_pro_features, x_pro_embeddings))\n",
    "        \n",
    "        #----------------------------------------------------------------------------\n",
    "        \n",
    "        # Create target array\n",
    "        y = np.concatenate([np.ones(self.batch_size), np.zeros(self.batch_size)])\n",
    "        \n",
    "        return (x_que, x_pro), y\n",
    "    \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Shuffle qa_data after each epoch\n",
    "        \"\"\"\n",
    "        if self.shuffle:\n",
    "            self.qa_data = shuffle(self.qa_data)\n",
    "    \n",
    "    \n",
    "    def __convert(self, batch):\n",
    "        \"\"\"\n",
    "        Computes embeddings for questions based on average of precomputed tag embeddings\n",
    "        and embeddings for professionals based on precomputed industry embeddings\n",
    "        \"\"\"\n",
    "        x_que, x_pro = [], []\n",
    "        \n",
    "        for que, pro in batch:\n",
    "            tmp = []\n",
    "            \n",
    "            for tag in self.que_tag.get(que, ['#']):\n",
    "                tmp.append(self.tag_emb.get(tag, np.zeros(10)))\n",
    "            x_que.append(np.vstack(tmp).mean(axis = 0))\n",
    "            x_pro.append(self.ind_emb.get(self.pro_ind[pro], np.zeros(10)))\n",
    "        \n",
    "        return np.vstack(x_que), np.vstack(x_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = BatchGenerator(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 25)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.__getitem__(0)[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.9 ms ± 114 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "generator.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%lprun -f generator.__getitem__ generator.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature list (added _ in file's name)\n",
    "with open('feature_list_.json', 'w') as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            'ques': list(x_ques.columns),\n",
    "            'prof': list(x_prof.columns)\n",
    "        }, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change last_answer_date feature for incorrect professionals\n",
    "self.prof_ques_dict = {prof_id:df_slice.sort_values(by='professionals_last_answer_date_time')\n",
    "                       for prof_id, df_slice in self.qa_data.groupby('professionals_id')}\n",
    "\n",
    "for i, prof in enumerate(new_profs):\n",
    "    prof_ques = self.prof_ques_dict[prof]\n",
    "    index = np.searchsorted(np.array(prof_ques.professionals_last_answer_date_time),\n",
    "                            negative_batch.professionals_last_answer_date_time.iloc[i])\n",
    "    if index < 1:\n",
    "        index = 1\n",
    "\n",
    "    negative_batch.iloc[i, 13:] = prof_ques.iloc[index-1, 13:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
