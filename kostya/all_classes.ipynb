{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DatasetCreator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetCreator:\n",
    "    \"\"\"\n",
    "    Class that imports initial datasets and creates additional datasets for convenience\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path='../../data/', created=False):\n",
    "        # Add data_path to class properties\n",
    "        self.data_path = data_path\n",
    "        \n",
    "        # Import all initial datasets\n",
    "        self.emails = pd.read_csv(data_path + 'emails.csv')\n",
    "        self.questions = pd.read_csv(data_path + 'questions.csv')\n",
    "        self.professionals = pd.read_csv(data_path + 'professionals.csv')\n",
    "        self.comments = pd.read_csv(data_path + 'comments.csv')\n",
    "        self.tag_users = pd.read_csv(data_path + 'tag_users.csv')\n",
    "        self.group_memberships = pd.read_csv(data_path + 'group_memberships.csv')\n",
    "        self.tags = pd.read_csv(data_path + 'tags.csv')\n",
    "        self.students = pd.read_csv(data_path + 'students.csv')\n",
    "        self.groups = pd.read_csv(data_path + 'groups.csv')\n",
    "        self.tag_questions = pd.read_csv(data_path + 'tag_questions.csv')\n",
    "        self.matches = pd.read_csv(data_path + 'matches.csv')\n",
    "        self.answers = pd.read_csv(data_path + 'answers.csv')\n",
    "        self.school_memberships = pd.read_csv(data_path + 'school_memberships.csv')\n",
    "        \n",
    "        if created:\n",
    "            # Load additional datasets from disk\n",
    "            self.qa_data = pd.read_csv(self.data_path + 'qa_data.csv')\n",
    "            self.prof_data = pd.read_csv(self.data_path + 'prof_data.csv')\n",
    "            self.stud_data = pd.read_csv(self.data_path + 'stud_data.csv')\n",
    "        else:\n",
    "            # Create additional datasets and save them to disk\n",
    "            self.additional_datasets_creation()\n",
    "    \n",
    "    \n",
    "    def additional_datasets_creation(self):\n",
    "        \"\"\"\n",
    "        Creates additional datasets for futher processing and save them to disk.\n",
    "        \"\"\"\n",
    "        # Create temporary dataset for further processing\n",
    "        all_data = self.all_data_creation()\n",
    "        \n",
    "        # Create question-answer pairs dataset called qa_data\n",
    "        self.qa_data = self.qa_data_creation(all_data)\n",
    "        \n",
    "        # Create dataset called prof_data compirising data of professionals\n",
    "        # who answered at least one question\n",
    "        self.prof_data = self.prof_data_creation(all_data)\n",
    "        \n",
    "        # Create dataset called stud_data compirising data of students\n",
    "        # who asked at least one answered question\n",
    "        self.stud_data = self.stud_data_creation(all_data)\n",
    "        \n",
    "        # Save new datasets to disc\n",
    "        self.qa_data.to_csv(self.data_path + 'qa_data.csv', index=False)\n",
    "        self.prof_data.to_csv(self.data_path + 'prof_data.csv', index=False)\n",
    "        self.stud_data.to_csv(self.data_path + 'stud_data.csv', index=False)\n",
    "    \n",
    "    \n",
    "    def all_data_creation(self):\n",
    "        \"\"\"\n",
    "        Merges questions, answers, professionals and students datasets\n",
    "        to get temporary dataset for further processing\n",
    "        \"\"\"\n",
    "        # Merge questions with answers and delete not answered questions\n",
    "        all_data = self.questions.merge(self.answers, how='right', left_on='questions_id', right_on='answers_question_id')\n",
    "        \n",
    "        # Merge with professionals and students (students asked, professionals answered)\n",
    "        # Maybe change this in the future by taking care of professional who change status to students and vise versa\n",
    "        all_data = all_data.merge(self.professionals, how='inner', left_on='answers_author_id', right_on='professionals_id')\n",
    "        all_data = all_data.merge(self.students, how='inner', left_on='questions_author_id', right_on='students_id')\n",
    "        \n",
    "        # Transform dates from string representation to datetime object\n",
    "        all_data.answers_date_added = pd.to_datetime(all_data.answers_date_added)\n",
    "        all_data.questions_date_added = pd.to_datetime(all_data.questions_date_added)\n",
    "        \n",
    "        # Add questions_age feature, which represents amount of time\n",
    "        # from question emergence to a particular answer to that question\n",
    "        all_data['questions_age'] = all_data.answers_date_added - all_data.questions_date_added\n",
    "        \n",
    "        # Delete html tags and extra spaces from question and answer body\n",
    "        all_data.questions_body = (all_data.questions_body\n",
    "                                   .apply(lambda x: re.sub(r'(<[^>]*[/]?>|[\\r]?\\n)', ' ', str(x)))\n",
    "                                   .apply(lambda x: re.sub(r' +', ' ', x).strip()))\n",
    "        all_data.answers_body = (all_data.answers_body\n",
    "                                 .apply(lambda x: re.sub(r'(<[^>]*[/]?>|[\\r]?\\n)', ' ', str(x)))\n",
    "                                 .apply(lambda x: re.sub(r' +', ' ', x).strip()))\n",
    "        \n",
    "        # Count the number of words in question and answer body and add two new features\n",
    "        all_data['questions_body_length'] = all_data.questions_body.apply(lambda x: len(x.split(' ')))\n",
    "        all_data['answers_body_length'] = all_data.answers_body.apply(lambda x: len(x.split(' ')))\n",
    "        \n",
    "        return all_data\n",
    "    \n",
    "    \n",
    "    def qa_data_creation(self, all_data):\n",
    "        \"\"\"\n",
    "        Creates question-answer pairs dataset called qa_data_data\n",
    "        \"\"\"\n",
    "        # Temporary qa_data representation\n",
    "        qa_data = all_data\n",
    "        \n",
    "        # Select only unique professionals\n",
    "        temp = qa_data[['professionals_id', 'answers_date_added', 'answers_id']]\n",
    "        prof_unique = pd.DataFrame(temp.professionals_id.unique(), columns=['professionals_id'])\n",
    "        prof_unique = prof_unique.merge(self.professionals, how='left', on='professionals_id')\n",
    "        \n",
    "        # For every professional add a \"dummy\" question with answer date being professional's registration date\n",
    "        prof_unique['answers_id'] = list(None for _ in range(prof_unique.shape[0]))\n",
    "        prof_unique['answers_date_added'] = prof_unique['professionals_date_joined']\n",
    "        prof_unique = prof_unique[['professionals_id', 'answers_date_added', 'answers_id']]\n",
    "        \n",
    "        # Add \"dummy\" questions to all questions\n",
    "        temp = pd.concat([temp, prof_unique])\n",
    "        \n",
    "        # Sort by professionals and answer dates\n",
    "        temp = temp.sort_values(by=['professionals_id', 'answers_date_added']).reset_index(drop=True)\n",
    "        \n",
    "        # Get the sorted representation of the answers_date_added and shift the index down by one\n",
    "        # so that current question is aligned with previous question answer date\n",
    "        last_answer_date = pd.DataFrame({'professionals_last_answer_date': temp.answers_date_added})\n",
    "        last_answer_date.index += 1\n",
    "        \n",
    "        # Add the professionals_last_answer_date column to temp\n",
    "        temp = temp.merge(last_answer_date, left_index=True, right_index=True)\n",
    "        temp.dropna(subset=['answers_id'], inplace=True)\n",
    "        temp.drop(columns=['professionals_id', 'answers_date_added'], inplace=True)\n",
    "        \n",
    "        # Add professionals_last_answer_date column to qa_data \n",
    "        qa_data = qa_data.merge(temp, on='answers_id')\n",
    "        \n",
    "        # Transform dates from string representation to datetime object\n",
    "        qa_data.professionals_last_answer_date = pd.to_datetime(qa_data.professionals_last_answer_date)\n",
    "        \n",
    "        # Final qa_data representation\n",
    "        qa_data = qa_data[[\n",
    "            'students_id', 'questions_id', 'questions_title', 'questions_body',\n",
    "            'questions_body_length', 'questions_date_added', 'professionals_id',\n",
    "            'answers_id', 'answers_body', 'answers_date_added', 'professionals_last_answer_date'\n",
    "        ]]\n",
    "        \n",
    "        return qa_data\n",
    "    \n",
    "    \n",
    "    def prof_data_creation(self, all_data):\n",
    "        \"\"\"\n",
    "        Creates dataset called prof_data compirising data of professionals who answered at least one question\n",
    "        \"\"\"\n",
    "        # Select only professionals who answered at least one question\n",
    "        active_professionals = pd.DataFrame({'professionals_id': all_data.professionals_id.unique()})\n",
    "        prof_data = self.professionals.merge(active_professionals, how='right', on='professionals_id')\n",
    "        \n",
    "        # Extract state or country from location\n",
    "        prof_data['professionals_state'] = prof_data['professionals_location'].apply(lambda loc: str(loc).split(', ')[-1])\n",
    "        \n",
    "        # Transform dates from string representation to datetime object\n",
    "        prof_data.professionals_date_joined = pd.to_datetime(prof_data.professionals_date_joined)\n",
    "        \n",
    "        # Count the number of answered questions by each professional\n",
    "        number_answered = all_data[['questions_id', 'professionals_id']].groupby('professionals_id').count()\n",
    "        number_answered = number_answered.rename({'questions_id': 'professionals_questions_answered'}, axis=1)\n",
    "        \n",
    "        # Add professionals_questions_answered feature to prof_data\n",
    "        prof_data = prof_data.merge(number_answered, left_on='professionals_id', right_index=True)\n",
    "        \n",
    "        # Get average question age for every professional among questions he answered\n",
    "        average_question_age = (\n",
    "            all_data.groupby('professionals_id')\n",
    "            .questions_age.mean(numeric_only=False)\n",
    "        )\n",
    "        average_question_age = pd.DataFrame({'professionals_average_question_age': average_question_age})\n",
    "        \n",
    "        # Add professionals_average_question_age feature to prof_data\n",
    "        prof_data = prof_data.merge(average_question_age, on='professionals_id')\n",
    "        \n",
    "        # Get all emails that every acting professional received\n",
    "        prof_emails_received = pd.merge(\n",
    "            prof_data[['professionals_id']], self.emails,\n",
    "            left_on='professionals_id', right_on='emails_recipient_id')\n",
    "        \n",
    "        # Get all questions every acting professional received in emails\n",
    "        prof_email_questions = prof_emails_received.merge(\n",
    "            self.matches, how='inner', left_on='emails_id', right_on='matches_email_id')\n",
    "        \n",
    "        # Get answered questions about which professionals were notified by email\n",
    "        questions_answered_from_emails = prof_email_questions.merge(\n",
    "            self.qa_data[['professionals_id', 'questions_id']],\n",
    "            left_on=['professionals_id', 'matches_question_id'],\n",
    "            right_on=['professionals_id', 'questions_id'])\n",
    "        \n",
    "        # Count the number of answered questions about which professionals were notified by email\n",
    "        email_activated = (questions_answered_from_emails\n",
    "                           .groupby('professionals_id')[['questions_id']].count()\n",
    "                           .rename(columns={'questions_id': 'professionals_email_activated'}))\n",
    "        \n",
    "        # Add professionals_email_activated feature to prof_data\n",
    "        # This feature is percent of answered questions about which professionals were notified by email\n",
    "        prof_data = prof_data.merge(email_activated, on='professionals_id', how='left')\n",
    "        prof_data.professionals_email_activated.fillna(0, inplace=True)\n",
    "        prof_data.professionals_email_activated /= prof_data.professionals_questions_answered\n",
    "        \n",
    "        # Compute average question and answer body length for each professional\n",
    "        average_question_body_length = all_data.groupby('professionals_id')[['questions_body_length']].mean().reset_index()\n",
    "        average_answer_body_length = all_data.groupby('professionals_id')[['answers_body_length']].mean().reset_index()\n",
    "        \n",
    "        # Add average question and answer body length features to prof_data\n",
    "        prof_data = (prof_data.merge(average_question_body_length, on='professionals_id')\n",
    "                     .rename(columns={'questions_body_length': 'professionals_average_question_body_length'}))\n",
    "        prof_data = (prof_data.merge(average_answer_body_length, on='professionals_id')\n",
    "                     .rename(columns={'answers_body_length': 'professionals_average_answer_body_length'}))\n",
    "        \n",
    "        return prof_data\n",
    "    \n",
    "    \n",
    "    def stud_data_creation(self, all_data):\n",
    "        \"\"\"\n",
    "        Creates dataset called stud_data compirising data of students who asked at least one answered question\n",
    "        \"\"\"\n",
    "        # Select only students who asked at least one answered question\n",
    "        active_students = pd.DataFrame({'students_id': all_data.students_id.unique()})\n",
    "        stud_data = self.students.merge(active_students, how='right', on='students_id')\n",
    "        \n",
    "        # Extract state or country from location\n",
    "        stud_data['students_state'] = stud_data['students_location'].apply(lambda loc: str(loc).split(', ')[-1])\n",
    "        \n",
    "        # Transform dates from string representation to datetime object\n",
    "        stud_data.students_date_joined = pd.to_datetime(stud_data.students_date_joined)\n",
    "        \n",
    "        # Count the number of asked questions by each student\n",
    "        number_asked = all_data[['questions_id', 'students_id']].groupby('students_id').count()\n",
    "        number_asked = number_asked.rename({'questions_id': 'students_questions_asked'}, axis=1)\n",
    "        \n",
    "        # Add students_questions_answered feature to stud_data\n",
    "        stud_data = stud_data.merge(number_asked, left_on='students_id', right_index=True)\n",
    "        \n",
    "        # Get average question age for every student among questions he asked that were answered\n",
    "        average_question_age = (\n",
    "            all_data.groupby('students_id')\n",
    "            .questions_age.mean(numeric_only=False)\n",
    "        )\n",
    "        average_question_age = pd.DataFrame({'students_average_question_age': average_question_age})\n",
    "        \n",
    "        # Add professionals_average_question_age feature to prof_data\n",
    "        stud_data = stud_data.merge(average_question_age, on='students_id')\n",
    "        \n",
    "        # Compute average question and answer body length for each student\n",
    "        average_question_body_length = all_data.groupby('students_id')[['questions_body_length']].mean().reset_index()\n",
    "        average_answer_body_length = all_data.groupby('students_id')[['answers_body_length']].mean().reset_index()\n",
    "        \n",
    "        # Add average question and answer body length features to stud_data\n",
    "        stud_data = (stud_data.merge(average_question_body_length, on='students_id')\n",
    "                     .rename(columns={'questions_body_length': 'students_average_question_body_length'}))\n",
    "        stud_data = (stud_data.merge(average_answer_body_length, on='students_id')\n",
    "                     .rename(columns={'answers_body_length': 'students_average_answer_body_length'}))\n",
    "        \n",
    "        return stud_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator = DatasetCreator(created=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>students_id</th>\n",
       "      <th>questions_id</th>\n",
       "      <th>questions_title</th>\n",
       "      <th>questions_body</th>\n",
       "      <th>questions_body_length</th>\n",
       "      <th>questions_date_added</th>\n",
       "      <th>professionals_id</th>\n",
       "      <th>answers_id</th>\n",
       "      <th>answers_body</th>\n",
       "      <th>answers_date_added</th>\n",
       "      <th>professionals_last_answer_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8f6f374ffd834d258ab69d376dd998f5</td>\n",
       "      <td>332a511f1569444485cf7a7a556a5e54</td>\n",
       "      <td>Teacher   career   question</td>\n",
       "      <td>What is a maths teacher? what is a maths teach...</td>\n",
       "      <td>14</td>\n",
       "      <td>2016-04-26 11:14:26</td>\n",
       "      <td>36ff3b3666df400f956f8335cf53e09e</td>\n",
       "      <td>4e5f01128cae4f6d8fd697cec5dca60c</td>\n",
       "      <td>Hi! You are asking a very interesting question...</td>\n",
       "      <td>2016-04-29 19:40:14</td>\n",
       "      <td>2016-04-29 14:15:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        students_id                      questions_id  \\\n",
       "0  8f6f374ffd834d258ab69d376dd998f5  332a511f1569444485cf7a7a556a5e54   \n",
       "\n",
       "               questions_title  \\\n",
       "0  Teacher   career   question   \n",
       "\n",
       "                                      questions_body  questions_body_length  \\\n",
       "0  What is a maths teacher? what is a maths teach...                     14   \n",
       "\n",
       "  questions_date_added                  professionals_id  \\\n",
       "0  2016-04-26 11:14:26  36ff3b3666df400f956f8335cf53e09e   \n",
       "\n",
       "                         answers_id  \\\n",
       "0  4e5f01128cae4f6d8fd697cec5dca60c   \n",
       "\n",
       "                                        answers_body  answers_date_added  \\\n",
       "0  Hi! You are asking a very interesting question... 2016-04-29 19:40:14   \n",
       "\n",
       "  professionals_last_answer_date  \n",
       "0            2016-04-29 14:15:00  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creator.qa_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os, pickle, json, re\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(DatasetCreator):\n",
    "    \"\"\"\n",
    "    Class for qa_data, prof_data and stud_data feature preprocessing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, created=False):\n",
    "        \"\"\"\n",
    "        Initializes DatasetCreator class and loads existing\n",
    "        preprocessors that were already fit to data\n",
    "        \"\"\"\n",
    "        # Initialize DatasetCreator\n",
    "        super().__init__(created=created)\n",
    "        \n",
    "        # Load existing preprocessors that were already fit to data\n",
    "        if os.path.isfile('preprocessors.pickle'):\n",
    "            with open('preprocessors.pickle', 'rb') as file:\n",
    "                self.pp = pickle.load(file)\n",
    "        else:\n",
    "            self.pp = {}\n",
    "        \n",
    "        # Load file that contains number of categories for categorical features\n",
    "        with open('cat_features.json') as f:\n",
    "            self.cat_features = json.load(f)\n",
    "        \n",
    "        # Carry out preprocessing of all datasets\n",
    "        self.qa_data_preprocessing()\n",
    "        self.prof_data_preprocessing()\n",
    "        self.stud_data_preprocessing()\n",
    "    \n",
    "    \n",
    "    def qa_data_preprocessing(self):\n",
    "        \"\"\"\n",
    "        Preprocesses qa_data dataset\n",
    "        \"\"\"\n",
    "        # Preprocess datetime and timedelta features\n",
    "        Preprocessor.datetime(self.qa_data, 'questions_date_added', hour=True)\n",
    "        Preprocessor.datetime(self.qa_data, 'answers_date_added', hour=True)\n",
    "        Preprocessor.datetime(self.qa_data, 'professionals_last_answer_date', hour=True)\n",
    "        \n",
    "        # Preprocess numerical features\n",
    "        for feature in [\n",
    "            'questions_date_added_time', 'questions_date_added_dow',\n",
    "            'answers_date_added_time', 'answers_date_added_dow',\n",
    "            'professionals_last_answer_date_time', 'professionals_last_answer_date_dow',\n",
    "            'questions_body_length',\n",
    "        ]:\n",
    "            Preprocessor.numerical(self.qa_data, feature, self.pp)\n",
    "    \n",
    "    \n",
    "    def prof_data_preprocessing(self):\n",
    "        \"\"\"\n",
    "        Preprocesses prof_data dataset\n",
    "        \"\"\"\n",
    "        # Preprocess datetime and timedelta features\n",
    "        Preprocessor.datetime(self.prof_data, 'professionals_date_joined')\n",
    "        Preprocessor.timedelta(self.prof_data, 'professionals_average_question_age')\n",
    "        \n",
    "        # Preprocess numerical features\n",
    "        for feature in [\n",
    "            'professionals_questions_answered', 'professionals_date_joined_time',\n",
    "            'professionals_date_joined_dow', 'professionals_average_question_age',\n",
    "            'professionals_average_question_body_length', 'professionals_average_answer_body_length',\n",
    "        ]:\n",
    "            Preprocessor.numerical(self.prof_data, feature, self.pp)\n",
    "        \n",
    "        # Will need textual representation of industry in BatchGenerator\n",
    "        self.prof_data['professionals_industry_textual'] = self.prof_data['professionals_industry']\n",
    "        \n",
    "        # Preprocess categorical features\n",
    "        Preprocessor.categorical(\n",
    "            self.prof_data, 'professionals_industry',\n",
    "            self.cat_features['n_cats']['prof']['professionals_industry'],\n",
    "            self.pp, oblige_fit=True\n",
    "        )\n",
    "        Preprocessor.categorical(\n",
    "            self.prof_data, 'professionals_location',\n",
    "            self.cat_features['n_cats']['prof']['professionals_location'],\n",
    "            self.pp, oblige_fit=True\n",
    "        )\n",
    "        Preprocessor.categorical(\n",
    "            self.prof_data, 'professionals_state',\n",
    "            self.cat_features['n_cats']['prof']['professionals_state'],\n",
    "            self.pp, oblige_fit=True\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def stud_data_preprocessing(self):\n",
    "        \"\"\"\n",
    "        Preprocesses stud_data dataset\n",
    "        \"\"\"\n",
    "        # Preprocess datetime and timedelta features\n",
    "        Preprocessor.datetime(self.stud_data, 'students_date_joined')\n",
    "        Preprocessor.timedelta(self.stud_data, 'students_average_question_age')\n",
    "        \n",
    "        # Preprocess numerical features\n",
    "        for feature in [\n",
    "            'students_questions_asked', 'students_date_joined_time',\n",
    "            'students_date_joined_dow', 'students_average_question_age',\n",
    "            'students_average_question_body_length', 'students_average_answer_body_length',\n",
    "        ]:\n",
    "            Preprocessor.numerical(self.stud_data, feature, self.pp)\n",
    "        \n",
    "        # Preprocess categorical features\n",
    "        Preprocessor.categorical(\n",
    "            self.stud_data, 'students_location',\n",
    "            self.cat_features['n_cats']['ques']['students_location'],\n",
    "            self.pp, oblige_fit=True\n",
    "        )\n",
    "        Preprocessor.categorical(\n",
    "            self.stud_data, 'students_state',\n",
    "            self.cat_features['n_cats']['ques']['students_state'],\n",
    "            self.pp, oblige_fit=True\n",
    "        )\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def datetime(df: pd.DataFrame, feature: str, hour: bool = False):\n",
    "        \"\"\"\n",
    "        Generates a bunch of new datetime features and drops the original feature inplace\n",
    "\n",
    "        :param df: Data to work with.\n",
    "        :param feature: Name of a column in df that contains date.\n",
    "        :param hour: Whether feature contains time.\n",
    "        \"\"\"\n",
    "        df[feature] = pd.to_datetime(df[feature])\n",
    "\n",
    "        df[feature + '_time'] = df[feature].apply(lambda d: d.year + d.dayofyear / 365)\n",
    "        df[feature + '_doy_sin'] = df[feature].apply(lambda d: np.sin(2 * np.pi * d.dayofyear / 365))\n",
    "        df[feature + '_doy_cos'] = df[feature].apply(lambda d: np.cos(2 * np.pi * d.dayofyear / 365))\n",
    "        df[feature + '_dow'] = df[feature].apply(lambda d: d.weekday())\n",
    "\n",
    "        if hour:\n",
    "            df[feature + '_hour_sin'] = df[feature].apply(lambda d: np.sin(2 * np.pi * (d.hour + d.minute / 60) / 24))\n",
    "            df[feature + '_hour_cos'] = df[feature].apply(lambda d: np.cos(2 * np.pi * (d.hour + d.minute / 60) / 24))\n",
    "\n",
    "        df.drop(columns=feature, inplace=True)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def timedelta(df: pd.DataFrame, feature: str):\n",
    "        \"\"\"\n",
    "        Generates the new timedelta feature\n",
    "\n",
    "        :param df: Data to work with.\n",
    "        :param feature: Name of a column in df that contains timedelta.\n",
    "        \"\"\"\n",
    "        df[feature] = pd.to_timedelta(df[feature])\n",
    "\n",
    "        df[feature] = df[feature] / pd.Timedelta(\"1 day\")\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_preprocessor(fit_data: np.array, feature: str, base, pp: dict, oblige_fit: bool):\n",
    "        \"\"\"\n",
    "        Creates new preprocessor having class base or uses existing one in preprocessors.pickle\n",
    "        Returns this preprocessor\n",
    "\n",
    "        :param fit_data: NumPy array of data to fit new preprocessor.\n",
    "        :param feature: Feature name to search for in preprocessors.pickle.\n",
    "        :param base: Preprocessor's class.\n",
    "        :param pp: Object with preprocessors.\n",
    "        :param oblige_fit: Whether to fit new preprocessor on feature even if there already exists one.\n",
    "        :returns: Preprocessor object.\n",
    "        \"\"\"    \n",
    "        if feature in pp and not oblige_fit:\n",
    "            preproc = pp[feature]\n",
    "        else:\n",
    "            preproc = base()\n",
    "            preproc.fit(fit_data)\n",
    "            pp[feature] = preproc\n",
    "            with open('preprocessors.pickle', 'wb') as file:\n",
    "                pickle.dump(pp, file)\n",
    "        return preproc\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def numerical(df: pd.DataFrame, feature: str, pp: dict, oblige_fit: bool = False):\n",
    "        \"\"\"\n",
    "        Transforms via StandardScaler\n",
    "\n",
    "        :param df: Data to work with.\n",
    "        :param feature: Name of a column in df that contains numerical data.\n",
    "        :param pp: Object with preprocessors.\n",
    "        :param oblige_fit: Whether to fit new StandardScaler on feature even if there already exists one.\n",
    "        \"\"\"\n",
    "        fit_data = df[feature].values.reshape(-1, 1).astype('float64')\n",
    "        sc = Preprocessor._get_preprocessor(fit_data, feature, StandardScaler, pp, oblige_fit)\n",
    "        df[feature] = sc.transform(fit_data)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def categorical(df: pd.DataFrame, feature: str, n: int, pp: dict, oblige_fit: bool = False):\n",
    "        \"\"\"\n",
    "        Encodes top n most popular values with different labels from 0 to n-1,\n",
    "        remaining values with n and NaNs with n+1\n",
    "\n",
    "        :param df: Data to work with.\n",
    "        :param feature: Name of a column in df that contains categorical data.\n",
    "        :param n: Number of top by popularity values to move in separate categories.\n",
    "                  0 to encode everything with different labels.\n",
    "        :param pp: Object with preprocessors.\n",
    "        :param oblige_fit: Whether to fit new LabelEncoder on feature even if there already exists one.\n",
    "        \"\"\"\n",
    "        vc = df[feature].value_counts()\n",
    "        n = len(vc) if n == 0 else n\n",
    "\n",
    "        top = set(vc[:n].index)\n",
    "        isin_top = df[feature].isin(top)\n",
    "\n",
    "        fit_data = df.loc[isin_top, feature]\n",
    "        le = Preprocessor._get_preprocessor(fit_data, feature, LabelEncoder, pp, oblige_fit)\n",
    "\n",
    "        isin_le = df[feature].isin(set(le.classes_))\n",
    "        df.loc[isin_le, feature] = le.transform(df.loc[isin_le, feature])\n",
    "\n",
    "        bottom = set(vc.index) - set(le.classes_)\n",
    "        isin_bottom = df[feature].isin(bottom)\n",
    "        df.loc[isin_bottom, feature] = n\n",
    "        df[feature].fillna(n + 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pp = Preprocessor(created=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>students_id</th>\n",
       "      <th>questions_id</th>\n",
       "      <th>questions_title</th>\n",
       "      <th>questions_body</th>\n",
       "      <th>questions_body_length</th>\n",
       "      <th>professionals_id</th>\n",
       "      <th>answers_id</th>\n",
       "      <th>answers_body</th>\n",
       "      <th>questions_date_added_time</th>\n",
       "      <th>questions_date_added_doy_sin</th>\n",
       "      <th>...</th>\n",
       "      <th>answers_date_added_doy_cos</th>\n",
       "      <th>answers_date_added_dow</th>\n",
       "      <th>answers_date_added_hour_sin</th>\n",
       "      <th>answers_date_added_hour_cos</th>\n",
       "      <th>professionals_last_answer_date_time</th>\n",
       "      <th>professionals_last_answer_date_doy_sin</th>\n",
       "      <th>professionals_last_answer_date_doy_cos</th>\n",
       "      <th>professionals_last_answer_date_dow</th>\n",
       "      <th>professionals_last_answer_date_hour_sin</th>\n",
       "      <th>professionals_last_answer_date_hour_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8f6f374ffd834d258ab69d376dd998f5</td>\n",
       "      <td>332a511f1569444485cf7a7a556a5e54</td>\n",
       "      <td>Teacher   career   question</td>\n",
       "      <td>What is a maths teacher? what is a maths teach...</td>\n",
       "      <td>-0.683257</td>\n",
       "      <td>36ff3b3666df400f956f8335cf53e09e</td>\n",
       "      <td>4e5f01128cae4f6d8fd697cec5dca60c</td>\n",
       "      <td>Hi! You are asking a very interesting question...</td>\n",
       "      <td>-0.401803</td>\n",
       "      <td>0.903356</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.474951</td>\n",
       "      <td>0.748932</td>\n",
       "      <td>-0.906308</td>\n",
       "      <td>0.422618</td>\n",
       "      <td>-0.646438</td>\n",
       "      <td>0.880012</td>\n",
       "      <td>-0.474951</td>\n",
       "      <td>0.771827</td>\n",
       "      <td>-0.55557</td>\n",
       "      <td>-0.83147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        students_id                      questions_id  \\\n",
       "0  8f6f374ffd834d258ab69d376dd998f5  332a511f1569444485cf7a7a556a5e54   \n",
       "\n",
       "               questions_title  \\\n",
       "0  Teacher   career   question   \n",
       "\n",
       "                                      questions_body  questions_body_length  \\\n",
       "0  What is a maths teacher? what is a maths teach...              -0.683257   \n",
       "\n",
       "                   professionals_id                        answers_id  \\\n",
       "0  36ff3b3666df400f956f8335cf53e09e  4e5f01128cae4f6d8fd697cec5dca60c   \n",
       "\n",
       "                                        answers_body  \\\n",
       "0  Hi! You are asking a very interesting question...   \n",
       "\n",
       "   questions_date_added_time  questions_date_added_doy_sin  \\\n",
       "0                  -0.401803                      0.903356   \n",
       "\n",
       "                    ...                     answers_date_added_doy_cos  \\\n",
       "0                   ...                                      -0.474951   \n",
       "\n",
       "   answers_date_added_dow  answers_date_added_hour_sin  \\\n",
       "0                0.748932                    -0.906308   \n",
       "\n",
       "   answers_date_added_hour_cos  professionals_last_answer_date_time  \\\n",
       "0                     0.422618                            -0.646438   \n",
       "\n",
       "   professionals_last_answer_date_doy_sin  \\\n",
       "0                                0.880012   \n",
       "\n",
       "   professionals_last_answer_date_doy_cos  professionals_last_answer_date_dow  \\\n",
       "0                               -0.474951                            0.771827   \n",
       "\n",
       "   professionals_last_answer_date_hour_sin  \\\n",
       "0                                 -0.55557   \n",
       "\n",
       "   professionals_last_answer_date_hour_cos  \n",
       "0                                 -0.83147  \n",
       "\n",
       "[1 rows x 26 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.qa_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BatchGenerator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras.utils import Sequence\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from utils import TextProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator(Sequence):\n",
    "    \"\"\"\n",
    "    Generates batches of data to feed into the model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pp: Preprocessor, batch_size: int = 50, shuffle: bool = True):\n",
    "        \"\"\"\n",
    "        Loads required datasets from pp, batch_size and shuffle parameters\n",
    "        \"\"\"\n",
    "        self.qa_data = pp.qa_data.merge(pp.stud_data, on='students_id')\n",
    "        self.prof_data = pp.prof_data\n",
    "        \n",
    "        # Select unique professionals from the ones that answered at least one question\n",
    "        self.unique_profs = pp.prof_data.professionals_id.unique()\n",
    "        \n",
    "        #----------------------------------------------------------------------------\n",
    "        #               INTEGRATION WITH NIKITA'S BATCH GENERATOR\n",
    "        #----------------------------------------------------------------------------\n",
    "        \n",
    "        # Load required datasets (their names are left as they were in Nikita's batch generator)\n",
    "        tag_que = pp.tag_questions\n",
    "        tags = pp.tags\n",
    "        pro = pp.prof_data\n",
    "        que = pp.qa_data\n",
    "        \n",
    "        # Import precomputed embeddings\n",
    "        with open('tags_embs.pickle', 'rb') as file:\n",
    "            self.tag_emb = pickle.load(file)\n",
    "        with open('industries_embs.pickle', 'rb') as file:\n",
    "            self.ind_emb = pickle.load(file)\n",
    "        \n",
    "        # Preprocess professionals industries\n",
    "        self.tp = TextProcessor()\n",
    "        \n",
    "        pro['professionals_industry_textual'] = pro['professionals_industry_textual'].apply(self.tp.process)\n",
    "        tags['tags_tag_name'] = tags['tags_tag_name'].apply(lambda x: self.tp.process(x, allow_stopwords=True))\n",
    "        \n",
    "        # Map professionals_id to professionals_industry_textual\n",
    "        self.pro_ind = {row['professionals_id']: row['professionals_industry_textual'] for i, row in pro.iterrows()}\n",
    "        \n",
    "        # Create string of tags for every question\n",
    "        que_tags = (que.merge(tag_que, left_on='questions_id', right_on='tag_questions_question_id')\n",
    "                       .merge(tags, left_on='tag_questions_tag_id', right_on='tags_tag_id'))\n",
    "        que_tags = (que_tags[['questions_id', 'tags_tag_name']]\n",
    "                    .groupby('questions_id', as_index=False)\n",
    "                    .aggregate(lambda x: ' '.join(x)))\n",
    "        \n",
    "        # Map questions_id to string of tags\n",
    "        self.que_tag = {row['questions_id']: row['tags_tag_name'].split() for i, row in que_tags.iterrows()}\n",
    "        \n",
    "        #----------------------------------------------------------------------------\n",
    "        \n",
    "        # Set batch_size and shuffle parameters\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # Initial shuffle \n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Denotes the number of batches per epoch\n",
    "        \"\"\"\n",
    "        return self.qa_data.shape[0] // (self.batch_size)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generates one batch of data\n",
    "        \"\"\"\n",
    "        # Positive batch is selected by index\n",
    "        positive_batch = self.qa_data.iloc[index * self.batch_size : (index + 1) * self.batch_size, :]\n",
    "        negative_batch = positive_batch\n",
    "        \n",
    "        # Choose random professionals for negative batch\n",
    "        cur_profs = negative_batch.professionals_id\n",
    "        new_profs = np.random.choice(self.unique_profs, self.batch_size)\n",
    "        \n",
    "        # Check if all professionals from negative batch are different from true professionals\n",
    "        while np.sum(cur_profs == new_profs) > 0:\n",
    "            # If not (tiny probability), resample random professionals\n",
    "            new_profs = np.random.choice(self.unique_profs, self.batch_size)\n",
    "        \n",
    "        # Assign random professionals to negative batch\n",
    "        negative_batch.assign(professionals_id=new_profs)\n",
    "        \n",
    "        # Concatenate positive and negative batches into a single batch\n",
    "        single_batch = pd.concat([positive_batch, negative_batch])\n",
    "        \n",
    "        # Add professionals data to single_batch\n",
    "        single_batch = single_batch.merge(self.prof_data, on='professionals_id')\n",
    "        \n",
    "        # Select statistical question features\n",
    "        x_que_features = single_batch[[\n",
    "            'students_location', 'students_state', 'students_questions_asked',\n",
    "            'students_average_question_age', 'students_average_question_body_length',\n",
    "            'students_average_answer_body_length',\n",
    "            \n",
    "            'students_date_joined_time', 'students_date_joined_doy_sin',\n",
    "            'students_date_joined_doy_cos', 'students_date_joined_dow',\n",
    "            \n",
    "            'questions_body_length',\n",
    "            \n",
    "            'questions_date_added_time', 'questions_date_added_doy_sin',\n",
    "            'questions_date_added_doy_cos', 'questions_date_added_dow',\n",
    "            'questions_date_added_hour_sin', 'questions_date_added_hour_cos',\n",
    "        ]].values\n",
    "        \n",
    "        # Select statistical professional features\n",
    "        x_pro_features = single_batch[[\n",
    "            'professionals_industry', 'professionals_location', 'professionals_state',\n",
    "            'professionals_questions_answered', 'professionals_average_question_age',\n",
    "            'professionals_average_question_body_length', 'professionals_average_answer_body_length',\n",
    "            'professionals_email_activated',\n",
    "            \n",
    "            'professionals_date_joined_time', 'professionals_date_joined_doy_sin',\n",
    "            'professionals_date_joined_doy_cos', 'professionals_date_joined_dow',\n",
    "            \n",
    "            'professionals_last_answer_date_time', 'professionals_last_answer_date_doy_sin',\n",
    "            'professionals_last_answer_date_doy_cos', 'professionals_last_answer_date_dow',\n",
    "            'professionals_last_answer_date_hour_sin', 'professionals_last_answer_date_hour_cos',\n",
    "        ]].values\n",
    "        \n",
    "        #----------------------------------------------------------------------------\n",
    "        #               INTEGRATION WITH NIKITA'S BATCH GENERATOR\n",
    "        #----------------------------------------------------------------------------\n",
    "        \n",
    "        # Extract embeddings from batch questions and professionals\n",
    "        x_que_embeddings, x_pro_embeddings = self.__convert(\n",
    "            single_batch[['questions_id', 'professionals_id']].values)\n",
    "        \n",
    "        # Stack statistical features and embeddings\n",
    "        x_que = np.hstack((x_que_features, x_que_embeddings))\n",
    "        x_pro = np.hstack((x_pro_features, x_pro_embeddings))\n",
    "        \n",
    "        #----------------------------------------------------------------------------\n",
    "        \n",
    "        # Create target array\n",
    "        y = np.concatenate([np.ones(self.batch_size), np.zeros(self.batch_size)])\n",
    "        \n",
    "        return [x_que, x_pro], y\n",
    "    \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Shuffle qa_data after each epoch\n",
    "        \"\"\"\n",
    "        if self.shuffle:\n",
    "            self.qa_data = shuffle(self.qa_data)\n",
    "    \n",
    "    \n",
    "    def __convert(self, batch):\n",
    "        \"\"\"\n",
    "        Computes embeddings for questions based on average of precomputed tag embeddings\n",
    "        and embeddings for professionals based on precomputed industry embeddings\n",
    "        \"\"\"\n",
    "        x_que, x_pro = [], []\n",
    "        for que, pro in batch:\n",
    "            tmp = []\n",
    "            for tag in self.que_tag.get(que, []):\n",
    "                tmp.append(self.tag_emb.get(tag, np.zeros(10)))\n",
    "            if len(tmp) == 0:\n",
    "                tmp.append(np.zeros(10))\n",
    "            x_que.append(np.vstack(tmp).mean(axis = 0))\n",
    "            x_pro.append(self.ind_emb.get(self.pro_ind[pro], np.zeros(10)))\n",
    "        \n",
    "        return np.vstack(x_que), np.vstack(x_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = BatchGenerator(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.__getitem__(0)[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.5 ms ± 161 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "generator.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%lprun -f generator.__getitem__ generator.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature list (added _ in file's name)\n",
    "with open('_feature_list.json', 'w') as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            'ques': list(x_ques.columns),\n",
    "            'prof': list(x_prof.columns)\n",
    "        }, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change last_answer_date feature for incorrect professionals\n",
    "self.prof_ques_dict = {prof_id:df_slice.sort_values(by='professionals_last_answer_date_time')\n",
    "                       for prof_id, df_slice in self.qa_data.groupby('professionals_id')}\n",
    "\n",
    "for i, prof in enumerate(new_profs):\n",
    "    prof_ques = self.prof_ques_dict[prof]\n",
    "    index = np.searchsorted(np.array(prof_ques.professionals_last_answer_date_time),\n",
    "                            negative_batch.professionals_last_answer_date_time.iloc[i])\n",
    "    if index < 1:\n",
    "        index = 1\n",
    "\n",
    "    negative_batch.iloc[i, 13:] = prof_ques.iloc[index-1, 13:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for other version of BatchGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49722, 30)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_data = pp.qa_data.merge(pp.stud_data, on='students_id')\n",
    "qa_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here without professionals_last_answer_date\n",
    "que_features = qa_data[[\n",
    "    'students_location', 'students_state', 'students_questions_asked',\n",
    "    'students_average_question_age', 'students_average_question_body_length',\n",
    "    'students_average_answer_body_length',\n",
    "\n",
    "    'students_date_joined_time', 'students_date_joined_doy_sin',\n",
    "    'students_date_joined_doy_cos', 'students_date_joined_dow',\n",
    "\n",
    "    'questions_body_length',\n",
    "\n",
    "    'questions_date_added_time', 'questions_date_added_doy_sin',\n",
    "    'questions_date_added_doy_cos', 'questions_date_added_dow',\n",
    "    'questions_date_added_hour_sin', 'questions_date_added_hour_cos',\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49722, 17)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "que_feature_dict = {que.loc[i]: que_features.loc[i].values for i in range(que.size)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_features = pp.prof_data[[\n",
    "    'professionals_industry', 'professionals_location', 'professionals_state',\n",
    "    'professionals_questions_answered', 'professionals_average_question_age',\n",
    "    'professionals_average_question_body_length', 'professionals_average_answer_body_length',\n",
    "    'professionals_email_activated',\n",
    "\n",
    "    'professionals_date_joined_time', 'professionals_date_joined_doy_sin',\n",
    "    'professionals_date_joined_doy_cos', 'professionals_date_joined_dow',\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro = pp.prof_data.professionals_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_feature_dict = {pro.loc[i]: pro_features.loc[i].values for i in range(pro.size)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('que_feature_dict.pickle', 'wb') as f:\n",
    "    pickle.dump(que_feature_dict, f)\n",
    "with open('pro_feature_dict.pickle', 'wb') as f:\n",
    "    pickle.dump(pro_feature_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last answer date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>students_id</th>\n",
       "      <th>questions_id</th>\n",
       "      <th>questions_title</th>\n",
       "      <th>questions_body</th>\n",
       "      <th>questions_body_length</th>\n",
       "      <th>professionals_id</th>\n",
       "      <th>answers_id</th>\n",
       "      <th>answers_body</th>\n",
       "      <th>questions_date_added_time</th>\n",
       "      <th>questions_date_added_doy_sin</th>\n",
       "      <th>...</th>\n",
       "      <th>answers_date_added_doy_cos</th>\n",
       "      <th>answers_date_added_dow</th>\n",
       "      <th>answers_date_added_hour_sin</th>\n",
       "      <th>answers_date_added_hour_cos</th>\n",
       "      <th>professionals_last_answer_date_time</th>\n",
       "      <th>professionals_last_answer_date_doy_sin</th>\n",
       "      <th>professionals_last_answer_date_doy_cos</th>\n",
       "      <th>professionals_last_answer_date_dow</th>\n",
       "      <th>professionals_last_answer_date_hour_sin</th>\n",
       "      <th>professionals_last_answer_date_hour_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8f6f374ffd834d258ab69d376dd998f5</td>\n",
       "      <td>332a511f1569444485cf7a7a556a5e54</td>\n",
       "      <td>Teacher   career   question</td>\n",
       "      <td>What is a maths teacher? what is a maths teach...</td>\n",
       "      <td>-0.683257</td>\n",
       "      <td>36ff3b3666df400f956f8335cf53e09e</td>\n",
       "      <td>4e5f01128cae4f6d8fd697cec5dca60c</td>\n",
       "      <td>Hi! You are asking a very interesting question...</td>\n",
       "      <td>-0.401803</td>\n",
       "      <td>0.903356</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.474951</td>\n",
       "      <td>0.748932</td>\n",
       "      <td>-0.906308</td>\n",
       "      <td>0.422618</td>\n",
       "      <td>-0.646438</td>\n",
       "      <td>0.880012</td>\n",
       "      <td>-0.474951</td>\n",
       "      <td>0.771827</td>\n",
       "      <td>-0.55557</td>\n",
       "      <td>-0.83147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        students_id                      questions_id  \\\n",
       "0  8f6f374ffd834d258ab69d376dd998f5  332a511f1569444485cf7a7a556a5e54   \n",
       "\n",
       "               questions_title  \\\n",
       "0  Teacher   career   question   \n",
       "\n",
       "                                      questions_body  questions_body_length  \\\n",
       "0  What is a maths teacher? what is a maths teach...              -0.683257   \n",
       "\n",
       "                   professionals_id                        answers_id  \\\n",
       "0  36ff3b3666df400f956f8335cf53e09e  4e5f01128cae4f6d8fd697cec5dca60c   \n",
       "\n",
       "                                        answers_body  \\\n",
       "0  Hi! You are asking a very interesting question...   \n",
       "\n",
       "   questions_date_added_time  questions_date_added_doy_sin  \\\n",
       "0                  -0.401803                      0.903356   \n",
       "\n",
       "                    ...                     answers_date_added_doy_cos  \\\n",
       "0                   ...                                      -0.474951   \n",
       "\n",
       "   answers_date_added_dow  answers_date_added_hour_sin  \\\n",
       "0                0.748932                    -0.906308   \n",
       "\n",
       "   answers_date_added_hour_cos  professionals_last_answer_date_time  \\\n",
       "0                     0.422618                            -0.646438   \n",
       "\n",
       "   professionals_last_answer_date_doy_sin  \\\n",
       "0                                0.880012   \n",
       "\n",
       "   professionals_last_answer_date_doy_cos  professionals_last_answer_date_dow  \\\n",
       "0                               -0.474951                            0.771827   \n",
       "\n",
       "   professionals_last_answer_date_hour_sin  \\\n",
       "0                                 -0.55557   \n",
       "\n",
       "   professionals_last_answer_date_hour_cos  \n",
       "0                                 -0.83147  \n",
       "\n",
       "[1 rows x 26 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.qa_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions_id</th>\n",
       "      <th>professionals_last_answer_date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>332a511f1569444485cf7a7a556a5e54</td>\n",
       "      <td>-0.646438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7a0d4bc67b1c492fb06fe455b1c07faf</td>\n",
       "      <td>0.781801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7a0d4bc67b1c492fb06fe455b1c07faf</td>\n",
       "      <td>0.781801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7a0d4bc67b1c492fb06fe455b1c07faf</td>\n",
       "      <td>-0.509311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0f1d6a4f276c4a05878dd48e03e52289</td>\n",
       "      <td>-0.450240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       questions_id  professionals_last_answer_date_time\n",
       "0  332a511f1569444485cf7a7a556a5e54                            -0.646438\n",
       "1  7a0d4bc67b1c492fb06fe455b1c07faf                             0.781801\n",
       "2  7a0d4bc67b1c492fb06fe455b1c07faf                             0.781801\n",
       "3  7a0d4bc67b1c492fb06fe455b1c07faf                            -0.509311\n",
       "4  0f1d6a4f276c4a05878dd48e03e52289                            -0.450240"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_last_answer_date = pp.qa_data[['questions_id', 'professionals_last_answer_date_time']]\n",
    "ans_date_added = pp.qa_data[['questions_id', 'answers_date_added_time']]\n",
    "pro_last_answer_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_date_added_dict = {que:date for que, date in ans_date_added.values}\n",
    "que_last_answer_date_dict = {que:date for que, date in que_last_answer_date.values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change last_answer_date feature for incorrect professionals\n",
    "pro_last_answer_dates_dict = {pro:df_slice.professionals_last_answer_date_time.sort_values().values\n",
    "                              for pro, df_slice in pp.qa_data.groupby('professionals_id')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('ans_date_added_dict.pickle', 'wb') as f:\n",
    "    pickle.dump(ans_date_added_dict, f)\n",
    "with open('que_last_answer_date_dict.pickle', 'wb') as f:\n",
    "    pickle.dump(que_last_answer_date_dict, f)\n",
    "with open('pro_last_answer_dates_dict.pickle', 'wb') as f:\n",
    "    pickle.dump(pro_last_answer_dates_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other version of BatchGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils import TextProcessor\n",
    "\n",
    "\n",
    "class BatchGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, pos_size, neg_size, data_path='../../data/'):\n",
    "        self.pos_size = pos_size\n",
    "        self.neg_size = neg_size\n",
    "        \n",
    "        que = pd.read_csv(data_path + 'questions.csv')\n",
    "        tag_que = pd.read_csv(data_path + 'tag_questions.csv')\n",
    "        tags = pd.read_csv(data_path + 'tags.csv')\n",
    "        pro = pd.read_csv(data_path + 'professionals.csv')\n",
    "        stu = pd.read_csv(data_path + 'students.csv')\n",
    "        ans = pd.read_csv(data_path + 'answers.csv')\n",
    "        \n",
    "        self.tp = TextProcessor()\n",
    "        pro['professionals_industry'] = pro['professionals_industry'].apply(self.tp.process)\n",
    "        tags['tags_tag_name'] = tags['tags_tag_name'].apply(lambda x: self.tp.process(x, allow_stopwords=True))\n",
    "        \n",
    "        self.pro_ind = {row['professionals_id']: row['professionals_industry'] for i, row in pro.iterrows()}\n",
    "        \n",
    "        que_tags = que.merge(tag_que, left_on = 'questions_id', right_on = 'tag_questions_question_id').merge(tags, left_on = 'tag_questions_tag_id', right_on = 'tags_tag_id')\n",
    "        que_tags = que_tags[['questions_id', 'tags_tag_name']].groupby(by = 'questions_id', as_index = False).aggregate(lambda x: ' '.join(x))\n",
    "        self.que_tag = {row['questions_id']: row['tags_tag_name'].split() for i, row in que_tags.iterrows()}\n",
    "        \n",
    "        ans_que = ans.merge(que, left_on = 'answers_question_id', right_on = 'questions_id')\n",
    "        ans_que_pro = ans_que.merge(pro, left_on = 'answers_author_id', right_on = 'professionals_id')\n",
    "        ans_que_pro = ans_que_pro.merge(stu, left_on = 'questions_author_id', right_on = 'students_id')\n",
    "        \n",
    "        self.ques = list(set(ans_que_pro['questions_id']))\n",
    "        self.pros = list(set(ans_que_pro['professionals_id']))\n",
    "        \n",
    "        self.que_pro_set = {(row['questions_id'], row['professionals_id']) for i, row in ans_que_pro.iterrows()}\n",
    "        self.que_pro_list = list(self.que_pro_set)\n",
    "        \n",
    "        with open('tags_embs.pickle', 'rb') as file:\n",
    "            self.tag_emb = pickle.load(file)\n",
    "        with open('industries_embs.pickle', 'rb') as file:\n",
    "            self.ind_emb = pickle.load(file)\n",
    "        \n",
    "        # Load que and pro statistical features\n",
    "        with open('que_feature_dict.pickle', 'rb') as f:\n",
    "            self.que_feature_dict = pickle.load(f)\n",
    "        with open('pro_feature_dict.pickle', 'rb') as f:\n",
    "            self.pro_feature_dict = pickle.load(f)\n",
    "        \n",
    "        # Load pro last answer dates dict and que answer date dict\n",
    "        with open('pro_last_answer_dates_dict.pickle', 'rb') as f:\n",
    "            self.pro_last_answer_dates_dict = pickle.load(f)\n",
    "        with open('ans_date_added_dict.pickle', 'rb') as f:\n",
    "            self.ans_date_added_dict = pickle.load(f)\n",
    "        with open('que_last_answer_date_dict.pickle', 'rb') as f:\n",
    "            self.que_last_answer_date_dict = pickle.load(f)\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.que_pro_list) // self.pos_size\n",
    "    \n",
    "    \n",
    "    def __convert(self, pairs):\n",
    "        x_que, x_pro = [], []\n",
    "        for que, pro in pairs:\n",
    "            tmp = []\n",
    "            for tag in self.que_tag.get(que, []):\n",
    "                tmp.append(self.tag_emb.get(tag, np.zeros(10)))\n",
    "            if len(tmp) == 0:\n",
    "                tmp.append(np.zeros(10))\n",
    "            \n",
    "            x_que.append(np.vstack(tmp).mean(axis = 0))\n",
    "            x_pro.append(self.ind_emb.get(self.pro_ind[pro], np.zeros(10)))\n",
    "        \n",
    "        return np.vstack(x_que), np.vstack(x_pro)\n",
    "    \n",
    "    \n",
    "    def __negative_que_last_answer_date(self, que, pro) -> (np.float64, bool):\n",
    "        ans_date = self.ans_date_added_dict[que]\n",
    "        pro_dates = self.pro_last_answer_dates_dict[pro]\n",
    "        \n",
    "        index = np.searchsorted(pro_dates, ans_date)\n",
    "        if index == 0:\n",
    "            return ans_date, False\n",
    "        \n",
    "        return pro_dates[index-1], True     \n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        pos_pairs = self.que_pro_list[self.pos_size * index: self.pos_size * (index + 1)]\n",
    "        neg_pairs = []\n",
    "        \n",
    "        pos_last_dates = []\n",
    "        neg_last_dates = []\n",
    "        \n",
    "        pos_que_features, pos_pro_features = [], []\n",
    "        neg_que_features, neg_pro_features = [], []\n",
    "        \n",
    "        for que, pro in pos_pairs:\n",
    "            pos_que_features.append(self.que_feature_dict[que])\n",
    "            pos_pro_features.append(self.pro_feature_dict[pro])\n",
    "            pos_last_dates.append(self.que_last_answer_date_dict[que])\n",
    "        \n",
    "        for i in range(self.neg_size):\n",
    "            que = np.random.choice(self.ques)\n",
    "            #------------------------------------------------------------------\n",
    "            #                    DISTRIBUTION COMPUTATION\n",
    "            #------------------------------------------------------------------\n",
    "            valid_pros = []\n",
    "            last_answer_dates = []\n",
    "            \n",
    "            # Add professionals and respective valid last answer dates to lists\n",
    "            for pro in self.pros:\n",
    "                if (que, pro) not in self.que_pro_set:\n",
    "                    last_date, valid_time = self.__negative_que_last_answer_date(que, pro)\n",
    "\n",
    "                    if not valid_time:\n",
    "                        continue\n",
    "\n",
    "                    valid_pros.append(pro)\n",
    "                    last_answer_dates.append(last_date)\n",
    "            \n",
    "            # Substact last answer dates from the actual date the answer was added\n",
    "            distances = self.ans_date_added_dict[que] - np.array(last_answer_dates)\n",
    "            \n",
    "            # Add approximately 1 hour to distances to avoid division by zero,\n",
    "            # apply log1p transformation to 1 / distances and normalize each entry\n",
    "            distances += 1e-4\n",
    "            distances = np.log1p(1 / distances)\n",
    "            distances /= distances.sum()\n",
    "            \n",
    "            # Sample one professional from distribution of distances\n",
    "            pro = np.random.choice(valid_pros, p=distances)\n",
    "            #-------------------------------------------------------------------\n",
    "            \n",
    "            # Add que and pro data to all required lists\n",
    "            last_date, _ = self.__negative_que_last_answer_date(que, pro)\n",
    "            neg_pairs.append((que, pro))\n",
    "            neg_que_features.append(self.que_feature_dict[que])\n",
    "            neg_pro_features.append(self.pro_feature_dict[pro])\n",
    "            neg_last_dates.append(last_date)\n",
    "        \n",
    "        pos_que_embeddings, pos_pro_embeddings = self.__convert(pos_pairs)\n",
    "        neg_que_embeddings, neg_pro_embeddings = self.__convert(neg_pairs)\n",
    "        \n",
    "        x_pos_que = np.hstack([np.array(pos_que_features), pos_que_embeddings])\n",
    "        x_neg_que = np.hstack([np.array(neg_que_features), neg_que_embeddings])\n",
    "        \n",
    "        # print(np.array(pos_pro_features).shape, np.array(pos_last_dates)[:, np.newaxis].shape, pos_pro_embeddings.shape)\n",
    "        x_pos_pro = np.hstack([np.array(pos_pro_features), np.array(pos_last_dates)[:, np.newaxis], pos_pro_embeddings])\n",
    "        x_neg_pro = np.hstack([np.array(neg_pro_features), np.array(neg_last_dates)[:, np.newaxis], neg_pro_embeddings])\n",
    "        \n",
    "        return [np.vstack([x_pos_que, x_neg_que]), np.vstack([x_pos_pro, x_neg_pro])], \\\n",
    "                np.vstack([np.ones((self.pos_size, 1)), np.zeros((self.neg_size, 1))])\n",
    "    \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.que_pro_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = BatchGenerator(64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 23)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator[0][0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.49 s ± 22.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "generator[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%lprun -f generator.__getitem__ generator.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
