{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, pickle\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create class for data preprocessing required for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetCreator:\n",
    "    \"\"\"\n",
    "    Class that imports initial datasets and creates additional datasets for convenience\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path='../../data/', created=False):\n",
    "        # Add data_path to class properties\n",
    "        self.data_path = data_path\n",
    "        \n",
    "        # Import all initial datasets\n",
    "        self.emails = pd.read_csv(data_path + 'emails.csv')\n",
    "        self.questions = pd.read_csv(data_path + 'questions.csv')\n",
    "        self.professionals = pd.read_csv(data_path + 'professionals.csv')\n",
    "        self.comments = pd.read_csv(data_path + 'comments.csv')\n",
    "        self.tag_users = pd.read_csv(data_path + 'tag_users.csv')\n",
    "        self.group_memberships = pd.read_csv(data_path + 'group_memberships.csv')\n",
    "        self.tags = pd.read_csv(data_path + 'tags.csv')\n",
    "        self.students = pd.read_csv(data_path + 'students.csv')\n",
    "        self.groups = pd.read_csv(data_path + 'groups.csv')\n",
    "        self.tag_questions = pd.read_csv(data_path + 'tag_questions.csv')\n",
    "        self.matches = pd.read_csv(data_path + 'matches.csv')\n",
    "        self.answers = pd.read_csv(data_path + 'answers.csv')\n",
    "        self.school_memberships = pd.read_csv(data_path + 'school_memberships.csv')\n",
    "        \n",
    "        if created:\n",
    "            # Load additional datasets from disk\n",
    "            self.qa_data = pd.read_csv(self.data_path + 'qa_data.csv')\n",
    "            self.prof_data = pd.read_csv(self.data_path + 'prof_data.csv')\n",
    "            self.stud_data = pd.read_csv(self.data_path + 'stud_data.csv')\n",
    "        else:\n",
    "            # Create additional datasets and save them to disk\n",
    "            self.additional_datasets_creation()\n",
    "    \n",
    "    \n",
    "    def additional_datasets_creation(self):\n",
    "        \"\"\"\n",
    "        Creates additional datasets for futher processing and save them to disk.\n",
    "        \"\"\"\n",
    "        # Create temporary dataset for further processing\n",
    "        all_data = self.all_data_creation()\n",
    "        \n",
    "        # Create question-answer pairs dataset called qa_data\n",
    "        self.qa_data = self.qa_data_creation(all_data)\n",
    "        \n",
    "        # Create dataset called prof_data compirising data of professionals\n",
    "        # who answered at least one question\n",
    "        self.prof_data = self.prof_data_creation(all_data)\n",
    "        \n",
    "        # Create dataset called stud_data compirising data of students\n",
    "        # who asked at least one answered question\n",
    "        self.stud_data = self.stud_data_creation(all_data)\n",
    "        \n",
    "        # Save new datasets to disc\n",
    "        self.qa_data.to_csv(self.data_path + 'qa_data.csv', index=False)\n",
    "        self.prof_data.to_csv(self.data_path + 'prof_data.csv', index=False)\n",
    "        self.stud_data.to_csv(self.data_path + 'stud_data.csv', index=False)\n",
    "    \n",
    "    \n",
    "    def all_data_creation(self):\n",
    "        \"\"\"\n",
    "        Merges questions, answers, professionals and students datasets\n",
    "        to get temporary dataset for further processing\n",
    "        \"\"\"\n",
    "        # Merge questions with answers and delete not answered questions\n",
    "        all_data = self.questions.merge(self.answers, how='right', left_on='questions_id', right_on='answers_question_id')\n",
    "        \n",
    "        # Merge with professionals and students (students asked, professionals answered)\n",
    "        # Maybe change this in the future by taking care of professional who change status to students and vise versa\n",
    "        all_data = all_data.merge(self.professionals, how='inner', left_on='answers_author_id', right_on='professionals_id')\n",
    "        all_data = all_data.merge(self.students, how='inner', left_on='questions_author_id', right_on='students_id')\n",
    "        \n",
    "        # Transform dates from string representation to datetime object\n",
    "        all_data.answers_date_added = pd.to_datetime(all_data.answers_date_added)\n",
    "        all_data.questions_date_added = pd.to_datetime(all_data.questions_date_added)\n",
    "        \n",
    "        # Add questions_age feature, which represents amount of time\n",
    "        # from question emergence to a particular answer to that question\n",
    "        all_data['questions_age'] = all_data.answers_date_added - all_data.questions_date_added\n",
    "        \n",
    "        return all_data\n",
    "    \n",
    "    \n",
    "    def qa_data_creation(self, all_data):\n",
    "        \"\"\"\n",
    "        Creates question-answer pairs dataset called qa_data_data\n",
    "        \"\"\"\n",
    "        # Temporary qa_data representation\n",
    "        qa_data = all_data\n",
    "        \n",
    "        # Select only unique professionals\n",
    "        temp = qa_data[['professionals_id', 'answers_date_added', 'answers_id']]\n",
    "        prof_unique = pd.DataFrame(temp.professionals_id.unique(), columns=['professionals_id'])\n",
    "        prof_unique = prof_unique.merge(self.professionals, how='left', on='professionals_id')\n",
    "        \n",
    "        # For every professional add a \"dummy\" question with answer date being professional's registration date\n",
    "        prof_unique['answers_id'] = list(None for _ in range(prof_unique.shape[0]))\n",
    "        prof_unique['answers_date_added'] = prof_unique['professionals_date_joined']\n",
    "        prof_unique = prof_unique[['professionals_id', 'answers_date_added', 'answers_id']]\n",
    "        \n",
    "        # Add \"dummy\" questions to all questions\n",
    "        temp = pd.concat([temp, prof_unique])\n",
    "        \n",
    "        # Sort by professionals and answer dates\n",
    "        temp = temp.sort_values(by=['professionals_id', 'answers_date_added']).reset_index(drop=True)\n",
    "        \n",
    "        # Get the sorted representation of the answers_date_added and shift the index down by one\n",
    "        # so that current question is aligned with previous question answer date\n",
    "        last_answer_date = pd.DataFrame({'professionals_last_answer_date': temp.answers_date_added})\n",
    "        last_answer_date.index += 1\n",
    "        \n",
    "        # Add the professionals_last_answer_date column to temp\n",
    "        temp = temp.merge(last_answer_date, left_index=True, right_index=True)\n",
    "        temp.dropna(subset=['answers_id'], inplace=True)\n",
    "        temp.drop(columns=['professionals_id', 'answers_date_added'], inplace=True)\n",
    "        \n",
    "        # Add professionals_last_answer_date column to qa_data \n",
    "        qa_data = qa_data.merge(temp, on='answers_id')\n",
    "        \n",
    "        # Transform dates from string representation to datetime object\n",
    "        qa_data.professionals_last_answer_date = pd.to_datetime(qa_data.professionals_last_answer_date)\n",
    "        \n",
    "        # Final qa_data representation\n",
    "        qa_data = qa_data[[\n",
    "            'students_id', 'questions_id', 'questions_title', 'questions_body',\n",
    "            'questions_date_added', 'professionals_id', 'answers_id', 'answers_body',\n",
    "            'professionals_last_answer_date'\n",
    "        ]]\n",
    "        \n",
    "        return qa_data\n",
    "    \n",
    "    \n",
    "    def prof_data_creation(self, all_data):\n",
    "        \"\"\"\n",
    "        Creates dataset called prof_data compirising data of professionals who answered at least one question\n",
    "        \"\"\"\n",
    "        # Select only professionals who answered at least one question\n",
    "        active_professionals = pd.DataFrame({'professionals_id': all_data.professionals_id.unique()})\n",
    "        prof_data = self.professionals.merge(active_professionals, how='right', on='professionals_id')\n",
    "        \n",
    "        # Extract state or country from location\n",
    "        prof_data['professionals_state'] = prof_data['professionals_location'].apply(lambda loc: str(loc).split(', ')[-1])\n",
    "        \n",
    "        # Transform dates from string representation to datetime object\n",
    "        prof_data.professionals_date_joined = pd.to_datetime(prof_data.professionals_date_joined)\n",
    "        \n",
    "        # Count the number of answered questions by each professional\n",
    "        number_answered = all_data[['questions_id', 'professionals_id']].groupby('professionals_id').count()\n",
    "        number_answered = number_answered.rename({'questions_id': 'professionals_questions_answered'}, axis=1)\n",
    "        \n",
    "        # Add professionals_questions_answered feature to prof_data\n",
    "        prof_data = prof_data.merge(number_answered, left_on='professionals_id', right_index=True)\n",
    "        \n",
    "        # Get average question age for every professional among questions he answered\n",
    "        average_question_age = (\n",
    "            all_data.groupby('professionals_id')\n",
    "            .questions_age.mean(numeric_only=False)\n",
    "        )\n",
    "        average_question_age = pd.DataFrame({'professionals_average_question_age': average_question_age})\n",
    "        \n",
    "        # Add professionals_average_question_age feature to prof_data\n",
    "        prof_data = prof_data.merge(average_question_age, on='professionals_id')\n",
    "        \n",
    "        return prof_data\n",
    "    \n",
    "    \n",
    "    def stud_data_creation(self, all_data):\n",
    "        \"\"\"\n",
    "        Creates dataset called stud_data compirising data of students who asked at least one answered question\n",
    "        \"\"\"\n",
    "        # Select only students who asked at least one answered question\n",
    "        active_students = pd.DataFrame({'students_id': all_data.students_id.unique()})\n",
    "        stud_data = self.students.merge(active_students, how='right', on='students_id')\n",
    "        \n",
    "        # Extract state or country from location\n",
    "        stud_data['students_state'] = stud_data['students_location'].apply(lambda loc: str(loc).split(', ')[-1])\n",
    "        \n",
    "        # Transform dates from string representation to datetime object\n",
    "        stud_data.students_date_joined = pd.to_datetime(stud_data.students_date_joined)\n",
    "        \n",
    "        # Count the number of asked questions by each student\n",
    "        number_asked = all_data[['questions_id', 'students_id']].groupby('students_id').count()\n",
    "        number_asked = number_asked.rename({'questions_id': 'students_questions_asked'}, axis=1)\n",
    "        \n",
    "        # Add students_questions_answered feature to stud_data\n",
    "        stud_data = stud_data.merge(number_asked, left_on='students_id', right_index=True)\n",
    "        \n",
    "        # Get average question age for every student among questions he asked that were answered\n",
    "        average_question_age = (\n",
    "            all_data.groupby('students_id')\n",
    "            .questions_age.mean(numeric_only=False)\n",
    "        )\n",
    "        average_question_age = pd.DataFrame({'students_average_question_age': average_question_age})\n",
    "        \n",
    "        # Add professionals_average_question_age feature to prof_data\n",
    "        stud_data = stud_data.merge(average_question_age, on='students_id')\n",
    "        \n",
    "        return stud_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator = DatasetCreator(created=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>students_id</th>\n",
       "      <th>students_location</th>\n",
       "      <th>students_date_joined</th>\n",
       "      <th>students_state</th>\n",
       "      <th>students_questions_asked</th>\n",
       "      <th>students_average_question_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12a89e96755a4dba83ff03e03043d9c0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-12-16 14:19:24</td>\n",
       "      <td>nan</td>\n",
       "      <td>2</td>\n",
       "      <td>794 days 06:00:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5bdd2eb44dd944a9a7ab9aba068d1ef2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-01 05:00:00</td>\n",
       "      <td>nan</td>\n",
       "      <td>2</td>\n",
       "      <td>2 days 16:05:06.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9658267bc2564a85bad1e802de5fb597</td>\n",
       "      <td>Wayne, Pennsylvania</td>\n",
       "      <td>2012-01-01 05:00:00</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>6</td>\n",
       "      <td>377 days 15:36:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7b1900c458e34573bfeb0d57ffbd260a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-01 05:00:00</td>\n",
       "      <td>nan</td>\n",
       "      <td>3</td>\n",
       "      <td>8 days 13:05:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e9efc4d6e06e49c7ae5afe1aad8c5bd5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-01 05:00:00</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>21 days 21:56:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        students_id    students_location students_date_joined  \\\n",
       "0  12a89e96755a4dba83ff03e03043d9c0                  NaN  2011-12-16 14:19:24   \n",
       "1  5bdd2eb44dd944a9a7ab9aba068d1ef2                  NaN  2012-01-01 05:00:00   \n",
       "2  9658267bc2564a85bad1e802de5fb597  Wayne, Pennsylvania  2012-01-01 05:00:00   \n",
       "3  7b1900c458e34573bfeb0d57ffbd260a                  NaN  2012-01-01 05:00:00   \n",
       "4  e9efc4d6e06e49c7ae5afe1aad8c5bd5                  NaN  2012-01-01 05:00:00   \n",
       "\n",
       "  students_state  students_questions_asked students_average_question_age  \n",
       "0            nan                         2             794 days 06:00:53  \n",
       "1            nan                         2        2 days 16:05:06.500000  \n",
       "2   Pennsylvania                         6             377 days 15:36:03  \n",
       "3            nan                         3               8 days 13:05:38  \n",
       "4            nan                         1              21 days 21:56:35  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creator.stud_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(DatasetCreator):\n",
    "    \"\"\"\n",
    "    Class for qa_data, prof_data and stud_data feature preprocessing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, created=False):\n",
    "        \"\"\"\n",
    "        Initializes DatasetCreator class and loads existing\n",
    "        preprocessors that were already fit to data\n",
    "        \"\"\"\n",
    "        # Initialize DatasetCreator\n",
    "        super().__init__(created=created)\n",
    "        \n",
    "        # Load existing preprocessors that were already fit to data\n",
    "        if os.path.isfile('preprocessors.pickle'):\n",
    "            with open('preprocessors.pickle', 'rb') as file:\n",
    "                self.pp = pickle.load(file)\n",
    "        else:\n",
    "            self.pp = {}\n",
    "        \n",
    "        # Carry out preprocessing of all datasets\n",
    "        self.qa_data_preprocessing()\n",
    "        self.prof_data_preprocessing()\n",
    "        self.stud_data_preprocessing()\n",
    "    \n",
    "    \n",
    "    def qa_data_preprocessing(self):\n",
    "        \"\"\"\n",
    "        Preprocesses qa_data dataset\n",
    "        \"\"\"\n",
    "        # Preprocess datetime and timedelta features\n",
    "        Preprocessor.datetime(self.qa_data, 'questions_date_added', hour=True)\n",
    "        Preprocessor.datetime(self.qa_data, 'professionals_last_answer_date', hour=True)\n",
    "        \n",
    "        # Preprocess numerical features\n",
    "        for feature in [\n",
    "            'questions_date_added_time', 'questions_date_added_doy_sin',\n",
    "            'professionals_last_answer_date_time', 'professionals_last_answer_date_dow'\n",
    "        ]:\n",
    "            Preprocessor.numerical(self.qa_data, feature, self.pp)\n",
    "    \n",
    "    \n",
    "    def prof_data_preprocessing(self):\n",
    "        \"\"\"\n",
    "        Preprocesses prof_data dataset\n",
    "        \"\"\"\n",
    "        # Preprocess datetime and timedelta features\n",
    "        Preprocessor.datetime(self.prof_data, 'professionals_date_joined')\n",
    "        Preprocessor.timedelta(self.prof_data, 'professionals_average_question_age')\n",
    "        \n",
    "        # Preprocess numerical features\n",
    "        for feature in [\n",
    "            'professionals_questions_answered', 'professionals_date_joined_time',\n",
    "            'professionals_date_joined_dow', 'professionals_average_question_age'\n",
    "        ]:\n",
    "            Preprocessor.numerical(self.prof_data, feature, self.pp)\n",
    "        \n",
    "        # Preprocess categorical features\n",
    "        Preprocessor.categorical(self.prof_data, 'professionals_location', 100, self.pp, oblige_fit=True)\n",
    "        Preprocessor.categorical(self.prof_data, 'professionals_state', 40, self.pp, oblige_fit=True)\n",
    "        Preprocessor.categorical(self.prof_data, 'professionals_industry', 100, self.pp, oblige_fit=True)\n",
    "    \n",
    "    \n",
    "    def stud_data_preprocessing(self):\n",
    "        \"\"\"\n",
    "        Preprocesses stud_data dataset\n",
    "        \"\"\"\n",
    "        # Preprocess datetime and timedelta features\n",
    "        Preprocessor.datetime(self.stud_data, 'students_date_joined')\n",
    "        Preprocessor.timedelta(self.stud_data, 'students_average_question_age')\n",
    "        \n",
    "        # Preprocess numerical features\n",
    "        for feature in [\n",
    "            'students_questions_asked', 'students_date_joined_time',\n",
    "            'students_date_joined_dow', 'students_average_question_age'\n",
    "        ]:\n",
    "            Preprocessor.numerical(self.stud_data, feature, self.pp)\n",
    "        \n",
    "        # Preprocess categorical features\n",
    "        Preprocessor.categorical(self.stud_data, 'students_location', 100, self.pp, oblige_fit=True)\n",
    "        Preprocessor.categorical(self.stud_data, 'students_state', 40, self.pp, oblige_fit=True)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def datetime(df: pd.DataFrame, feature: str, hour: bool = False):\n",
    "        \"\"\"\n",
    "        Generates a bunch of new datetime features and drops the original feature inplace\n",
    "\n",
    "        :param df: Data to work with.\n",
    "        :param feature: Name of a column in df that contains date.\n",
    "        :param hour: Whether feature contains time.\n",
    "        \"\"\"\n",
    "        df[feature] = pd.to_datetime(df[feature])\n",
    "\n",
    "        df[feature + '_time'] = df[feature].apply(lambda d: d.year + d.dayofyear / 365)\n",
    "        df[feature + '_doy_sin'] = df[feature].apply(lambda d: np.sin(2 * np.pi * d.dayofyear / 365))\n",
    "        df[feature + '_doy_cos'] = df[feature].apply(lambda d: np.cos(2 * np.pi * d.dayofyear / 365))\n",
    "        df[feature + '_dow'] = df[feature].apply(lambda d: d.weekday())\n",
    "\n",
    "        if hour:\n",
    "            df[feature + '_hour_sin'] = df[feature].apply(lambda d: np.sin(2 * np.pi * (d.hour + d.minute / 60) / 24))\n",
    "            df[feature + '_hour_cos'] = df[feature].apply(lambda d: np.cos(2 * np.pi * (d.hour + d.minute / 60) / 24))\n",
    "\n",
    "        df.drop(columns=feature, inplace=True)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def timedelta(df: pd.DataFrame, feature: str):\n",
    "        \"\"\"\n",
    "        Generates the new timedelta feature\n",
    "\n",
    "        :param df: Data to work with.\n",
    "        :param feature: Name of a column in df that contains timedelta.\n",
    "        \"\"\"\n",
    "        df[feature] = pd.to_timedelta(df[feature])\n",
    "\n",
    "        df[feature] = df[feature] / pd.Timedelta(\"1 day\")\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_preprocessor(fit_data: np.array, feature: str, base, pp: dict, oblige_fit: bool):\n",
    "        \"\"\"\n",
    "        Creates new preprocessor having class base or uses existing one in preprocessors.pickle\n",
    "        Returns this preprocessor\n",
    "\n",
    "        :param fit_data: NumPy array of data to fit new preprocessor.\n",
    "        :param feature: Feature name to search for in preprocessors.pickle.\n",
    "        :param base: Preprocessor's class.\n",
    "        :param pp: Object with preprocessors.\n",
    "        :param oblige_fit: Whether to fit new preprocessor on feature even if there already exists one.\n",
    "        :returns: Preprocessor object.\n",
    "        \"\"\"    \n",
    "        if feature in pp and not oblige_fit:\n",
    "            preproc = pp[feature]\n",
    "        else:\n",
    "            preproc = base()\n",
    "            preproc.fit(fit_data)\n",
    "            pp[feature] = preproc\n",
    "            with open('preprocessors.pickle', 'wb') as file:\n",
    "                pickle.dump(pp, file)\n",
    "        return preproc\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def numerical(df: pd.DataFrame, feature: str, pp: dict, oblige_fit: bool = False):\n",
    "        \"\"\"\n",
    "        Transforms via StandardScaler\n",
    "\n",
    "        :param df: Data to work with.\n",
    "        :param feature: Name of a column in df that contains numerical data.\n",
    "        :param pp: Object with preprocessors.\n",
    "        :param oblige_fit: Whether to fit new StandardScaler on feature even if there already exists one.\n",
    "        \"\"\"\n",
    "        fit_data = df[feature].values.reshape(-1, 1).astype('float64')\n",
    "        sc = Preprocessor._get_preprocessor(fit_data, feature, StandardScaler, pp, oblige_fit)\n",
    "        df[feature] = sc.transform(fit_data)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def categorical(df: pd.DataFrame, feature: str, n: int, pp: dict, oblige_fit: bool = False):\n",
    "        \"\"\"\n",
    "        Encodes top n most popular values with different labels from 0 to n-1,\n",
    "        remaining values with n and NaNs with n+1\n",
    "\n",
    "        :param df: Data to work with.\n",
    "        :param feature: Name of a column in df that contains categorical data.\n",
    "        :param n: Number of top by popularity values to move in separate categories.\n",
    "                  0 to encode everything with different labels.\n",
    "        :param pp: Object with preprocessors.\n",
    "        :param oblige_fit: Whether to fit new LabelEncoder on feature even if there already exists one.\n",
    "        \"\"\"\n",
    "        vc = df[feature].value_counts()\n",
    "        n = len(vc) if n == 0 else n\n",
    "\n",
    "        top = set(vc[:n].index)\n",
    "        isin_top = df[feature].isin(top)\n",
    "\n",
    "        fit_data = df.loc[isin_top, feature]\n",
    "        le = Preprocessor._get_preprocessor(fit_data, feature, LabelEncoder, pp, oblige_fit)\n",
    "\n",
    "        isin_le = df[feature].isin(set(le.classes_))\n",
    "        df.loc[isin_le, feature] = le.transform(df.loc[isin_le, feature])\n",
    "\n",
    "        bottom = set(vc.index) - set(le.classes_)\n",
    "        isin_bottom = df[feature].isin(bottom)\n",
    "        df.loc[isin_bottom, feature] = n\n",
    "        df[feature].fillna(n + 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pp = Preprocessor(created=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pp, batch_size=50, shuffle=True):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.qa_data = pp.qa_data.merge(pp.stud_data, on='students_id')\n",
    "        self.prof_data = pp.prof_data\n",
    "        self.unique_profs = pp.prof_data.professionals_id.unique()\n",
    "        \n",
    "        #self.prof_ques_dict = {prof_id:df_slice.sort_values(by='professionals_last_answer_date_time')\n",
    "        #                       for prof_id, df_slice in self.qa_data.groupby('professionals_id')}\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Denotes the number of batches per epoch\n",
    "        \"\"\"\n",
    "        return self.qa_data.shape[0] // (self.batch_size)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generates one batch of data\n",
    "        \"\"\"\n",
    "        positive_batch = self.qa_data.iloc[index * self.batch_size : (index + 1) * self.batch_size, :]\n",
    "        negative_batch = positive_batch\n",
    "        \n",
    "        cur_profs = negative_batch.professionals_id\n",
    "        new_profs = np.random.choice(self.unique_profs, self.batch_size)\n",
    "        \n",
    "        while np.sum(cur_profs == new_profs) > 0:\n",
    "            new_profs = np.random.choice(self.unique_profs, self.batch_size)\n",
    "        \n",
    "        negative_batch.assign(professionals_id=new_profs)\n",
    "        \n",
    "        \"\"\"\n",
    "        for i, prof in tqdm(enumerate(new_profs)):\n",
    "            prof_ques = self.prof_ques_dict[prof]\n",
    "            index = np.searchsorted(np.array(prof_ques.professionals_last_answer_date_time),\n",
    "                                    negative_batch.professionals_last_answer_date_time.iloc[i])\n",
    "            if index < 1:\n",
    "                index = 1\n",
    "            \n",
    "            negative_batch.iloc[i, 13:] = prof_ques.iloc[index-1, 13:]\n",
    "        \"\"\"\n",
    "        \n",
    "        single_batch = pd.concat([positive_batch, negative_batch])\n",
    "        single_batch = single_batch.merge(self.prof_data, on='professionals_id')\n",
    "        \n",
    "        x_ques = single_batch[\n",
    "            ['students_location', 'students_state',\n",
    "            'students_questions_asked', 'students_average_question_age'] + \\\n",
    "            list(single_batch.loc[:, 'students_date_joined_time':'students_date_joined_dow'].columns) + \\\n",
    "            list(single_batch.loc[:, 'questions_date_added_time':'questions_date_added_hour_cos'].columns)\n",
    "        ]\n",
    "        \n",
    "        x_prof = single_batch[\n",
    "            ['professionals_industry', 'professionals_location', 'professionals_state',\n",
    "            'professionals_questions_answered', 'professionals_average_question_age'] + \\\n",
    "            list(single_batch.loc[:, 'professionals_date_joined_time':'professionals_date_joined_dow'].columns) + \\\n",
    "            list(single_batch.loc[:, 'professionals_last_answer_date_time':'professionals_last_answer_date_hour_cos'].columns)\n",
    "        ]\n",
    "        \n",
    "        y = np.concatenate([np.ones(self.batch_size), np.zeros(self.batch_size)])\n",
    "        \n",
    "        return [x_ques.values, x_prof.values], y\n",
    "    \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Shuffle qa_data after each epoch\n",
    "        \"\"\"\n",
    "        if self.shuffle == True:\n",
    "            self.qa_data = shuffle(self.qa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = DataGenerator(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.2 ms ± 83.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "generator.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%lprun -f generator.__getitem__ generator.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
