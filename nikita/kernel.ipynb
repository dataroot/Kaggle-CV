{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are in <3 with neural networks and decided to try to build recommendation system via supervised problem approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set some global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/' # path to folder with initial .csv data files\n",
    "dump_path = 'dump/' # path to all the dump data, like saved models, calculated embeddings etc.\n",
    "split_date = '2019-01-01' # date used for splitting data on train and test subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data and split it into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = dict(), dict() # dictionaries with data split in train and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var, file_name, date_col in [('que', 'questions.csv', 'questions_date_added'), ('ans', 'answers.csv', 'answers_date_added'),\n",
    "                       ('pro', 'professionals.csv', 'professionals_date_joined'), ('stu', 'students.csv', 'students_date_joined')]:\n",
    "    df = pd.read_csv(data_path + file_name) # read the data\n",
    "\n",
    "    df[date_col] = pd.to_datetime(df[date_col]) # convert it to datetime64 type\n",
    "\n",
    "    train[var] = df[df[date_col] < split_date] # just to make sure no data from train will be present in test\n",
    "    test[var] = df # we will need to use all the data in test\n",
    "    \n",
    "tag_que = pd.read_csv(data_path + 'tag_questions.csv')\n",
    "tags = pd.read_csv(data_path + 'tags.csv').merge(tag_que, left_on='tags_tag_id', right_on='tag_questions_tag_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T14:15:33.870580Z",
     "start_time": "2019-04-11T14:15:33.745609Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'doc2vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ccfd5ec89c13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdoc2vec\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpipeline_d2v\u001b[0m \u001b[1;31m# pipeline for training and saving embeddings for\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m                                              \u001b[1;31m# professional's industries and question's tags via doc2vec algorithm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'doc2vec'"
     ]
    }
   ],
   "source": [
    "from doc2vec import pipeline as pipeline_d2v # pipeline for training and saving embeddings for\n",
    "                                             # professional's industries and question's tags via doc2vec algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*some text*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering and data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the main points of our data preparation:  \n",
    "- Questions and professionals are the two main entities in our recommendation system\n",
    "- Question's features are designed to be time-independent, while some of student's and professional's features inevitably depend on time. This is why we will need to compute student's and professional's feature vectors for each moment in time when they change. These moments will correspond to appearance of new answer\n",
    "- Student's features will be included in question's features later on, just before the model  \n",
    "\n",
    "So, there are three entities in our dataset for which we will compute features separately: question, student and professional. Each \n",
    "of three resulting DataFrames will contain features for each id on different timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_structure.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We distinguish three main types of features: categorical, numerical and datetime-like\n",
    "- For categorical features, we will consider only top N of its most popular categories. We will encode them via LabelEncoder with labels from 0 to N-1; all the remaining categories will be encoded with N and NaNs with N+1 label. Later, in model we will train embeddings for every label of each categorical feature\n",
    "- In numerical feature, we will fill its NaNs with either zero or mean and then standardize it via StandardScaler\n",
    "- From the datetime-like feature we will extract three new features: absolute time, sine and cosine of scaled day of the year. Then, we will work with three new features just like with numerical ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oblige_fit = True # whether it is necessary to fit new StandardScaler (for numerical features)\n",
    "                  # or LabelEncoder (for categorical) or use existent if there is one\n",
    "                  # True in train mode, False in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processors import QueProc, StuProc, ProProc # data preprocessors for questions, students and professionals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact question's features\n",
    "- Numerical\n",
    "    - Question's body length in symbols\n",
    "- Datetime-like\n",
    "    - Date question was added\n",
    "- Averaged question's tags embeddings pre-trained via doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "que_proc = QueProc(oblige_fit, dump_path)\n",
    "que_data = que_proc.transform(data['que'], tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "que_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student's features\n",
    "- Categorical\n",
    "    - Location\n",
    "    - State - extracted from location\n",
    "- Numerical\n",
    "    - Number of asked questions\n",
    "    - Average question age - time between question's date added and first answer\n",
    "    - Average asked question body length\n",
    "    - Average body length of answer on student's questions\n",
    "    - Average number of answers on student's questions\n",
    "- Datetime-like\n",
    "    - Student's date joined\n",
    "    - Time of previous student's question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stu_proc = StuProc(oblige_fit, dump_path)\n",
    "stu_data = stu_proc.transform(data['stu'], data['que'], data['ans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stu_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Professional's features\n",
    "- Categorical\n",
    "    - Industry\n",
    "    - Location\n",
    "    - State - extracted from location\n",
    "- Numerical\n",
    "    - Number of answered questions\n",
    "    - Average answered question's body length\n",
    "    - Average answer's body length\n",
    "- Datetime-like\n",
    "    - Professional's date joined\n",
    "    - Time of previous professional's answer\n",
    "- Industry embedding pre-trained via doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_proc = ProProc(oblige_fit, dump_path)\n",
    "pro_data = pro_proc.transform(data['pro'], data['que'], data['ans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional data computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general solution to problem is to build the classifier which for given question and professional will predict whether professional will answer to given question  \n",
    "Then we will partially use it to find simillar questions, make recommendations, etc.  \n",
    "So, for training classifier on binary classification task, we will need both positive and negative samples  \n",
    "First ones are easy to obtain: they are formed from those questions and professionals, where professional gave answer to that question\n",
    "So, we can compute positive pairs directly from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct dataframe used to extract positive pairs\n",
    "df = data['que'].merge(data['ans'], left_on='questions_id', right_on='answers_question_id') \\\n",
    "    .merge(data['pro'], left_on='answers_author_id', right_on='professionals_id') \\\n",
    "    .merge(data['stu'], left_on='questions_author_id', right_on='students_id')\n",
    "# select only relevant columns\n",
    "df = df[['questions_id', 'students_id', 'professionals_id']]\n",
    "# extract positive pairs themselves\n",
    "pos_pairs = list(df.itertuples(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mappings from professional's id to his registration date. Used in batch generator\n",
    "pro_dates = {row['professionals_id']: row['professionals_date_joined'] for i, row in data['pro'].iterrows()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch generator and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative samples are a bit more tricky. Logic of sampling negative question-professional pairs is implemented in batch generator. Some of its key points are:  \n",
    "- To determine the exact feature vectors of both students and professionals, we need the concept of current time\n",
    "- For positive professional-question pairs, current time is the time of an answer that connects given question and professional\n",
    "- In case of negative pairs, we will sample current time as random shift from question's added date  \n",
    "\n",
    "So, for sampling negative pair, we will choose random question, sample random current time and sample random professional among those who we registered at current time and who were not forming positive pair with selected question  \n",
    "Finally, for given tuple of question, student and professional, we will use their features at a current time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import BatchGenerator # class to ingest data from pre-processed DataFrames to model in form of batches of NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = BatchGenerator(que_data, stu_data, pro_data, 64, pos_pairs, nonneg_pairs,\n",
    "                            que_proc.pp['questions_date_added_time'], pro_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it comes to model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T17:12:23.458996Z",
     "start_time": "2019-04-11T17:12:23.413887Z"
    }
   },
   "source": [
    "*some text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Mothership, \\ # main model which combines two encoders (for questions and professionals)\n",
    "                   Adam          # and Keras optimizer to train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mothership(que_dim=len(que_data.columns) - 2 + len(stu_data.columns) - 2 + 1, # -2 is for id and time columns, \n",
    "                                                                                      # +1 is for current time feature\n",
    "                               que_input_embs=[102, 42], que_output_embs=[2, 2],\n",
    "                               pro_dim=len(pro_data.columns) - 2 + 1,\n",
    "                               pro_input_embs=[102, 102, 42], pro_output_embs=[2, 2, 2], inter_dim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(bg, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(dump_path + 'model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import permutation_importance, \\ # calculate model feature importance via random permutations of feature values\n",
    "                       plot_fi # and nicely plot it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*some text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy batch generator used to extract single big batch of data to calculate feature importance\n",
    "bg = BatchGenerator(que_data, stu_data, pro_data, 512, pos_pairs, nonneg_pairs,\n",
    "                    que_proc.pp['questions_date_added_time'], pro_dates)\n",
    "# dict with descriptions of feature names, used for visualization of feature importance\n",
    "fn = {\"que\": list(stu_data.columns[2:]) + list(que_data.columns[2:]) + ['que_current_time'],\n",
    "      \"pro\": list(pro_data.columns[2:]) + ['pro_current_time'],\n",
    "      'text': [f'que_emb_{i}' for i in range(10)] + [f'pro_emb_{i}' for i in range(10)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate and plot feature importance\n",
    "fi = permutation_importance(model, bg[0][0][0], bg[0][0][1], bg[0][1], fn, n_trials=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fi(fi, fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
