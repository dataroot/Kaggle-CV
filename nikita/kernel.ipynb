{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are in <3 with neural networks and decided to try to build recommendation system via supervised problem approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processors import QueProc, StuProc, ProProc # data preprocessors for questions, students and professionals\n",
    "from generator import BatchGenerator # class to ingest data from pre-processed DataFrames to model in form of batches of NumPy arrays\n",
    "from models import Mothership, \\ # main model which combines two encoders (for questions and professionals)\n",
    "                   Adam # and Keras optimizer to train it\n",
    "from evaluation import permutation_importance, \\ # calculate model feature importance via random permutations of feature values\n",
    "                       plot_fi # and nicely plot it\n",
    "from doc2vec import pipeline as pipeline_d2v # pipeline for training and saving embeddings for\n",
    "                                             # professional's industries and question's tags via doc2vec algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set some global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/' # path to folder with initial .csv data files\n",
    "dump_path = 'dump/' # path to all the dump data, like saved models, calculated embeddings etc.\n",
    "split_date = '2019-01-01' # date used for splitting data on train and test subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data and split it into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = dict(), dict() # dictionaries with data split in train and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var, file_name in [('que', 'questions.csv'), ('ans', 'answers.csv'),\n",
    "                       ('pro', 'professionals.csv'), ('stu', 'students.csv')]:\n",
    "    df = pd.read_csv(data_path + file_name) # read the data\n",
    "\n",
    "    date_col = [col for col in df.columns if 'date' in col][0] # find the column which contains date\n",
    "    df[date_col] = pd.to_datetime(df[date_col]) # convert it to datetime64 type\n",
    "\n",
    "    train[var] = df[df[date_col] < split_date] # just to make sure no data from train will be present in test\n",
    "    test[var] = df # we will need to use all the data in test\n",
    "    \n",
    "tag_que = pd.read_csv(data_path + 'tag_questions.csv')\n",
    "tags = pd.read_csv(data_path + 'tags.csv').merge(tag_que, left_on='tags_tag_id', right_on='tag_questions_tag_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering and data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the main points of our data preparation:  \n",
    "- Questions and professionals are the two main entities in our recommendation system\n",
    "- Student's features will be included in question's features\n",
    "- Question's features are designed to be time-independent, while some of student's and professional's features inevitably depend on time. This is why we will need to compute student's and professional's feature vectors for each moment in time when they change. These moments will correspond to appearance of new answer\n",
    "- So, there are three entities in our dataset for which we will compute features separately: question, student and professional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oblige_fit = True # whether it is necessary to fit new StandardScaler (for numerical features)\n",
    "                  # or LabelEncoder (for categorical) or use existent if there is one\n",
    "                  # True in train mode, False in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "que_proc = QueProc(oblige_fit, dump_path)\n",
    "que_data = que_proc.transform(data['que'], tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stu_proc = StuProc(oblige_fit, dump_path)\n",
    "stu_data = stu_proc.transform(data['stu'], data['que'], data['ans'])\n",
    "\n",
    "pro_proc = ProProc(oblige_fit, dump_path)\n",
    "pro_data = pro_proc.transform(data['pro'], data['que'], data['ans'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional data computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general solution to problem is to build the classifier which for given question and professional will predict whether professional will answer to given question  \n",
    "So, for training classifier on binary classification task, we will need both positive and negative samples  \n",
    "First ones are easy to obtain: we can compute them directly from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct dataframe used to extract positive pairs\n",
    "df = data['que'].merge(data['ans'], left_on='questions_id', right_on='answers_question_id') \\\n",
    "    .merge(data['pro'], left_on='answers_author_id', right_on='professionals_id') \\\n",
    "    .merge(data['stu'], left_on='questions_author_id', right_on='students_id')\n",
    "# select only relevant columns\n",
    "df = df[['questions_id', 'students_id', 'professionals_id']]\n",
    "# extract positive pairs themselves\n",
    "pos_pairs = list(df.itertuples(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mappings from professional's id to his registration date. Used in batch generator\n",
    "pro_dates = {row['professionals_id']: row['professionals_date_joined'] for i, row in data['pro'].iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
